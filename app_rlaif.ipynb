{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix6Sc4hVIkS5"
      },
      "source": [
        "# Reinforcement Learning from Human Feedback (RLHF)\n",
        "\n",
        "## Enhancing T5-Base Summarization with Proximal Policy Optimization (PPO) and PEFT Fine-Tuning\n",
        "\n",
        "\n",
        "Reinforcement Learning from Human Feedback, commonly known as **RLHF**, is a specialized machine learning approach that amalgamates traditional reinforcement learning techniques and human expertise. This union offers a unique pathway to training artificial intelligence agents.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "1. **Nature of RLAIF**: RLAIF can be understood as an iterative procedure. The system undergoes continuous improvement, adapting its learning function based on newly acquired human feedback.\n",
        "  \n",
        "2. **Safety and Trust**: Incorporating human feedback ensures the system not only comprehends the tasks it should execute but also recognizes actions it should avoid. This dual capability fosters safer and more trustworthy systems.\n",
        "  \n",
        "3. **Performance Enhancements**: A study in 2022 evidenced that RLHF outperforms conventional supervised learning (SL). This superiority can be attributed to RLHF's ability to assess cumulative rewards for coherent conversations, a nuanced understanding that SL misses.\n",
        "\n",
        "---\n",
        "\n",
        "RLHF has proven instrumental in guiding language models, molding them to align better with intricate human values. As we venture into this notebook, we'll deep-dive into the methodologies and applications of RLHF.\n",
        "\n",
        "\n",
        "\n",
        "Useful references:\n",
        "\n",
        "https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\n",
        "\n",
        "https://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-ppo-deeplearning-ai\n",
        "\n",
        "https://github.com/huggingface/trl/blob/main/tests/test_ppo_trainer.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlLA4cCfIkS8"
      },
      "source": [
        "Reward model: ideally a SequenceClassification type of model: We will use Bert\n",
        "\n",
        "Policy model: ideally a Seq2SeqLM: We will use T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgKBEr4-IkS8"
      },
      "source": [
        "![Alt text](https://github.com/PanoEvJ/summarization_RLHF/blob/main/image-1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQl4mRLeIkS8"
      },
      "source": [
        "Image source: https://huggingface.co/docs/trl/index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goJ54k4KIkS9"
      },
      "source": [
        "## Process Overview\n",
        "\n",
        "In this notebook, we embark on the journey of aligning a model using Reinforcement Learning from Human Feedback (RLHF). We'll employ various specialized models and leverage a structured training loop for this purpose.\n",
        "\n",
        "---\n",
        "\n",
        "### Models Utilized:\n",
        "\n",
        "1. **Rewards Model**:\n",
        "   - A finely-tuned model designated for dispensing rewards based on the actions of the policy model.\n",
        "\n",
        "2. **Base Model (Policy Model)**:\n",
        "   - The core model we aim to align using RLHF.\n",
        "   - During the RL process, this model becomes the \"policy model\", driving decisions and actions.\n",
        "\n",
        "3. **Reference Model**:\n",
        "   - A frozen replica of the base model.\n",
        "   - Its primary role is to act as a benchmark, monitoring the evolution of the policy model throughout the RL process.\n",
        "\n",
        "---\n",
        "\n",
        "### Training loop Overview:\n",
        "\n",
        "We begin by initializing the Proximal Policy Optimization (PPO) training class. The training process encompasses the following steps:\n",
        "\n",
        "- **Generation of Summaries**:\n",
        "  - Derived from the policy model.\n",
        "  \n",
        "- **Reward Assignment**:\n",
        "  - The generated summaries are channeled through the rewards model.\n",
        "  - Based on these summaries, rewards are determined, reflecting the alignment of the policy model with human preferences.\n",
        "  \n",
        "- **Model Adjustment via PPO**:\n",
        "  - Utilizing the acquired rewards, PPO refines the weights of the policy model, nudging it closer to human preferences.\n",
        "  \n",
        "This iterative training loop continues for a predefined number of steps.\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation:\n",
        "\n",
        "Post-training, we evaluate the efficacy and alignment of the policy model post-RL to determine its proficiency in mirroring human preferences.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hdy2lpPIkS9"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee-7XQrPIkS-"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q trl\n",
        "!pip install -q peft\n",
        "!pip install -q numpy\n",
        "!pip install -q pandas\n",
        "!pip install -q tqdm\n",
        "!pip install -q openai\n",
        "!pip install -q wandb\n",
        "!pip install -U -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE2XRxq2IkTB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "\n",
        "from peft import PeftModel, PeftConfig,  TaskType\n",
        "\n",
        "from peft import (\n",
        "    get_peft_config,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftType,\n",
        "    LoraConfig,\n",
        ")\n",
        "\n",
        "# AutoModelForCausalLMWithValueHead & AutoModelForSeq2SeqLMWithValueHead: A transformer model with an additional scalar output for each token which can be used as a value function in reinforcement learning.\n",
        "# https://huggingface.co/docs/trl/models#trl.AutoModelForSeq2SeqLMWithValueHead\n",
        "\n",
        "# trl: Transformer Reinforcement Learning library\n",
        "import trl\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead # https://huggingface.co/docs/trl/quickstart\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "# import evaluate\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tqdm library makes the loops show a smart progress meter.\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWvCQ2nIkTC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HKCUJR6IkTD"
      },
      "source": [
        "## GPT-3.5-turbo as the Rewards Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3QQbkzsIkTD"
      },
      "source": [
        "![Alt text](https://github.com/PanoEvJ/summarization_RLHF/blob/main/image-2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BSI7OoiIkTD"
      },
      "source": [
        "Image source: https://huggingface.co/blog/rlhf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJrgYOAfIkTD"
      },
      "source": [
        "## Reward Model in Reinforcement Learning (RL)\n",
        "\n",
        "In RL, a **reward model** is a mechanism providing feedback to the agent about its performance in its environment. Instead of predefined reward functions, reward models infer the reward signal from human feedback, especially useful in complex scenarios where crafting a reward function is challenging.\n",
        "\n",
        "### Why is it Important?\n",
        "\n",
        "- **Feedback Mechanism**: It's how agents determine if actions are beneficial or detrimental.\n",
        "- **Facilitates Learning**: Agents use these signals to update their policies to maximize rewards.\n",
        "- **Handles Complexity**: For real-world problems where explicit reward functions are difficult, a learned reward model is valuable.\n",
        "- **Safety and Alignment**: They ensure RL agents' objectives align with human intentions, reducing potential harmful behaviors.\n",
        "\n",
        "In our code, we're initializing a reward model (based on a transformer like BERT) for RL with Human Feedback (RLHF). This model generates reward signals from the agent's interactions, steering its learning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "openai_api_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "# 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"
      ],
      "metadata": {
        "id": "VcYx683EOioM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the summarization dataset."
      ],
      "metadata": {
        "id": "Seah1FLP4CNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dataset = load_dataset('CarperAI/openai_summarize_comparisons', split='test')"
      ],
      "metadata": {
        "id": "TjNaIdoVa9nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print one row of the dataset. There are 3 columns: prompt, chosen, rejected"
      ],
      "metadata": {
        "id": "2w0asSys4QZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dataset[10000]"
      ],
      "metadata": {
        "id": "4zTrDq6nbXih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test-run: score a random data point (prompt/summary) using ChatGPT."
      ],
      "metadata": {
        "id": "5SWqtksj4mrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_text       = orig_dataset[10000]['prompt']\n",
        "summarized_text = orig_dataset[10000]['chosen']"
      ],
      "metadata": {
        "id": "SFfGhPqSX-xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"### FULL TEXT:\\n {full_text} \\n\n",
        "### SUMMARIZED TEXT: \\n {summarized_text}\"\"\""
      ],
      "metadata": {
        "id": "8oIpJxmWeEVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "id": "qpaTXyzkeENm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    temperature = 0.9,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": \"\"\"You are an expert in text summarization. Below, you are given the full text and its summarization.\n",
        "Your role is to rate the provided summarization with scores ranging from 0 to 1, where: 0 is the lowest score, 1 is the highest score.\n",
        "Your response should only contain the rating score.\"\"\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}]\n",
        ")"
      ],
      "metadata": {
        "id": "DyEj-l3LRont"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "pvhNPu_Db69E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RLHF Fine-Tuning"
      ],
      "metadata": {
        "id": "_5gETCZqOfKU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek0klND7IkTF"
      },
      "source": [
        "## Loading the T5 Model for RLHF Fine-Tuning\n",
        "\n",
        "### Overview:\n",
        "\n",
        "T5, short for \"Text-to-Text Transfer Transformer\", is a state-of-the-art model designed to handle various text-to-text tasks. In this section, we'll be loading a T5 model that is intended to be fine-tuned using the Reinforcement Learning with Human Feedback (RLHF) approach.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Model Selection**:\n",
        "    - We've selected the T5 model for our fine-tuning process. Specifically, we'll be working with the \"t5-base\" variant which offers a balance between computational efficiency and performance.\n",
        "\n",
        "2. **Loading Model and Tokenizer**:\n",
        "    - `policy_model_path`: Specifies the directory path where our pre-trained (or fine-tuned) T5 model is saved.\n",
        "    - `policy_model_name`: Indicates the model name, which in this case is \"t5-base\".\n",
        "    - Using the `T5ForConditionalGeneration.from_pretrained` method, we load the model weights from our specified path.\n",
        "    - Similarly, the corresponding tokenizer, which is essential for converting text into a format that the T5 model can understand, is loaded using the `T5Tokenizer.from_pretrained` method.\n",
        "\n",
        "3. **Device Allocation**:\n",
        "    - The model is assigned to a computation device (either CPU or GPU) using the `.to(device)` method. This ensures efficient computation, especially when working with large datasets.\n",
        "\n",
        "### Test the Model:\n",
        "\n",
        "After loading, it's a good practice to perform some inference tests to ensure that the model is loaded correctly and is functioning as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnt__ajIkTF"
      },
      "outputs": [],
      "source": [
        "policy_model_path = \"JuanKO/rlhf_base_model\"\n",
        "policy_model_name = \"t5-base\"\n",
        "\n",
        "policy_model = T5ForConditionalGeneration.from_pretrained(policy_model_path)\n",
        "policy_model.to(device)\n",
        "policy_tokenizer = T5Tokenizer.from_pretrained(policy_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko1giYNwIkTG"
      },
      "source": [
        "### Testing the T5 Model for Summarization\n",
        "\n",
        "\n",
        "After loading our T5 model, we'll test its summarization capabilities on a sample text from the r/relationships subreddit. This test will help us understand the model's performance and its readiness for RLHF fine-tuning.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Setting the Task Prefix**:\n",
        "    - We use the prefix \"summarize: \" to indicate to the T5 model the type of task we want it to perform.\n",
        "\n",
        "2. **Sample Text**:\n",
        "    - We have selected a post from the r/relationships subreddit to be summarized. This text provides context about a user's relationship concerns related to her bisexuality.\n",
        "\n",
        "3. **Generating the Summary**:\n",
        "    - We feed the concatenated task prefix and text into our T5 model.\n",
        "    - The model then processes this input and returns a concise summary. The `generate` function is used to obtain this output, and we've set a max length of 100 tokens for our summary.\n",
        "\n",
        "4. **Decoding the Summary**:\n",
        "    - The output from the T5 model is in the form of token IDs. Using the T5 tokenizer's `decode` method, we convert these tokens back into human-readable text.\n",
        "\n",
        "5. **Scoring the Summary using the Reward Model**:\n",
        "    - With the generated summary in hand, we then use our previously defined `score_summaries` function to evaluate the quality of the summary.\n",
        "    - This function returns a score and logit value for both the chosen summary and a rejected (blank) summary. Higher scores and logits suggest better alignment with what the reward model considers a good summary.\n",
        "\n",
        "### Results:\n",
        "\n",
        "By examining the printed scores and logits, we can gauge the perceived quality of the generated summary according to our reward model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtwGg1SiIkTG"
      },
      "outputs": [],
      "source": [
        "# task_prefix = \"summarize: \"\n",
        "\n",
        "# text = \"SUBREDDIT: r/relationships TITLE: How do I/do I at all [20 F] tell my boyfriend [23 M] that I'm bisexual? POST: I've had two serious relationships prior to this one, both with women. They had no problem with me being bisexual and it was something known before the relationship -- my first girlfriend was also bisexual. I am now in a relationship with a guy. We've been exclusive for about a month. Having never faced this issue, I come to you, Reddit. Is this something that he needs to know? Is it really relevant to a hetero relationship, regardless of if one of the participants in the relationship is bisexual? If you guys think it is necessary, when do you think is the right time? I think my biggest fear is losing him because of it. I know that I should be with someone who is fine with who I am, but I really like the guy and I'd hate for my sexual orientation to be the thing that kills this.\"\n",
        "# #text = \"SUBREDDIT: r/legaladvice TITLE: What can I do legally to restore water to my condominium!? POST: Hi, I live in SE Michigan in a condominium complex. Our water was shut off due to non-payment. (we recieved no notice) and we had to pay all that was due ($1500) We payed this yesterday at 2, they said the water would be turned on immediately. It wasn't. It's now the next day. The lady in our assosciation keeps insisting that the water meter is in another condo. Which we can't access because the person living there is never there (it's being rented) Now we're stuck with no water, no shower, no teeth brushing, no toilets, and no food for certain meals.... Please help us... What can we do? We called the police and they say that we can file a civil report for the lady not doing her job...\"\n",
        "# prompt = f\"{task_prefix}{text}\"\n",
        "# input_ids = policy_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "# outputs = policy_model.generate(input_ids, max_length=100).to(device)\n",
        "\n",
        "# strOutput = policy_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# print(strOutput)\n",
        "\n",
        "# chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(rm_model, rm_tokenizer, strOutput, \"\")\n",
        "\n",
        "# print(f\"Chosen Score: {chosen_score:.4f}\")\n",
        "# print(f\"Rejected Score: {rejected_score:.4f}\")\n",
        "\n",
        "# print(f\"Chosen Logit: {chosen_logit:.4f}\")\n",
        "# print(f\"Rejected Logit: {rejected_logit:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hf5XfrIkTG"
      },
      "source": [
        "## Preparing the T5 Model for Peft + LoRA\n",
        "\n",
        "### Overview:\n",
        "\n",
        "Peft and LoRA (Low-Rank Adaptation) are techniques that enable efficient fine-tuning of pre-trained models by introducing low-rank structures into the models. Here, we'll configure the T5 model for this process.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Setting up the LoRA Configuration**:\n",
        "    - `LoraConfig` provides the configuration settings for Low-Rank Adaptation.\n",
        "        - `r`: Rank of the low-rank structure. In this instance, it's set to 8.\n",
        "        - `lora_alpha`: Scaling factor for the newly introduced low-rank parameters.\n",
        "        - `target_modules`: Specifies which parts of the model to apply LoRA. Here, we're targeting the \"q\" (query) and \"v\" (value) modules.\n",
        "        - `lora_dropout`: Dropout rate for the low-rank parameters. Set to 0.10, or 10%.\n",
        "        - `bias`: Specifies the type of bias for the low-rank projection. We've chosen \"none\" in this case.\n",
        "        - `task_type`: Indicates the type of task. As we're using T5, the task type is set to `SEQ_2_SEQ_LM`.\n",
        "\n",
        "2. **Applying LoRA Configuration to T5**:\n",
        "    - Using the `get_peft_model` function, we apply the LoRA configuration to our pre-loaded T5 model.\n",
        "    - The returned model (`policy_peft_model`) is equipped with the Peft + LoRA modifications and is ready for fine-tuning.\n",
        "\n",
        "### Summary of this section:\n",
        "\n",
        "Our T5 model is now prepared with Peft + LoRA adjustments. This configuration optimizes the model for more efficient fine-tuning on specific tasks while leveraging the powerful pre-trained knowledge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtiqUbdPIkTH"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.10,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # T5\n",
        ")\n",
        "\n",
        "policy_peft_model = get_peft_model(policy_model, lora_config)\n",
        "policy_peft_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z86DOZdIkTH"
      },
      "source": [
        "### Analyzing Trainable Parameters in the Peft + LoRA Configured T5 Model\n",
        "\n",
        "After applying the Peft + LoRA configuration to our T5 model, it's essential to inspect the model's parameters to understand its structure better.\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "1. **Trainable Parameters**:\n",
        "    - This refers to the parameters that will be updated during the training process.\n",
        "    - In our configured model, there are **884,736** trainable parameters.\n",
        "\n",
        "2. **Total Parameters**:\n",
        "    - This indicates the complete count of parameters present in the model, including those that are non-trainable.\n",
        "    - The model consists of **223,788,288** total parameters.\n",
        "\n",
        "3. **Percentage of Trainable Parameters**:\n",
        "    - It's useful to know the fraction of the model's parameters that are trainable, as this can influence training time and model flexibility.\n",
        "    - Only about **0.3953%** (or roughly 0.4%) of the entire model's parameters are trainable.\n",
        "\n",
        "### Summary of this section:\n",
        "\n",
        "The Peft + LoRA configuration results in a model where only a small fraction of parameters are trainable. This approach offers a balance, as it allows for specific fine-tuning while leveraging a vast pre-trained structure. The advantage is that it can lead to faster training times and might prevent overfitting, especially when training data is limited.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_SFOc-1IkTH"
      },
      "outputs": [],
      "source": [
        "policy_peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxANAubDIkTH"
      },
      "source": [
        "![Alt text](https://github.com/PanoEvJ/summarization_RLHF/blob/main/image-3.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYfI_3wkIkTH"
      },
      "source": [
        "Image source: https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sth-fk0qIkTI"
      },
      "source": [
        "## Instantiating the PPO Model with Value Head\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a reinforcement learning algorithm. In this step, we set up the model for PPO training using our earlier `policy_peft_model`.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **AutoModelForSeq2SeqLMWithValueHead**:\n",
        "    - An extension of the transformers model that includes a scalar output for each token, aiding in reinforcement learning.\n",
        "    - This model can capture the value function, an estimate of future rewards.\n",
        "\n",
        "2. **Inputs**:\n",
        "    - We pass in our `policy_peft_model`, which has been configured with Peft + LoRA, as the foundation for our PPO model.\n",
        "    - We set `torch_dtype` to `torch.bfloat16` for numerical precision and memory efficiency.\n",
        "    - The `is_trainable` flag is set to `True`, allowing us to further fine-tune the model using our RL loop.\n",
        "\n",
        "3. **Device Assignment**:\n",
        "    - We transfer our instantiated model to the appropriate device (`device`) for computation, ensuring efficient training.\n",
        "\n",
        "### Summary of this section:\n",
        "\n",
        "With our PPO model instantiated, we're poised to fine-tune our summarization model using reinforcement learning with human feedback. This approach is aimed at improving the model's performance in generating summaries based on human preferences and judgments.\n",
        "\n",
        "[More on PPO and TRL](https://huggingface.co/docs/trl/quickstart)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txdIgFSkIkTI"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/trl/quickstart\n",
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(policy_peft_model,\n",
        "                                                               torch_dtype=torch.bfloat16,\n",
        "                                                               is_trainable=True)\n",
        "\n",
        "ppo_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB5F3vzyIkTI"
      },
      "source": [
        "### Defining the Reference Model\n",
        "\n",
        "In reinforcement learning, especially when fine-tuning models using methods like Proximal Policy Optimization (PPO), it's helpful to have a reference model. This model represents the initial state or behavior of the learner model (in this case, the Language Model) before any alignment or optimization. It aids in calculating the importance sampling ratio, a critical component for stable and effective updates in PPO.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **create_reference_model**:\n",
        "    - A function provided by Huggingface's TRL (Transformer Reinforcement Learning) library.\n",
        "    - Creates a duplicate of the passed model which acts as a reference during the RL fine-tuning process.\n",
        "\n",
        "2. **Inputs**:\n",
        "    - The `policy_model` we previously defined serves as the input. This model acts as the basis for our reference model.\n",
        "\n",
        "3. **Device Assignment**:\n",
        "    - Once instantiated, we move our reference model to the specified device (`device`) for computations.\n",
        "\n",
        "### Summary of this section:\n",
        "\n",
        "By defining a reference model, we set a stable baseline against which we can measure and guide the progress and changes of our main model during the reinforcement learning process.\n",
        "\n",
        "[More on TRL and Reference Models](https://huggingface.co/docs/trl/models#trl.create_reference_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl2ukCSBIkTI"
      },
      "outputs": [],
      "source": [
        "ref_model = create_reference_model(policy_model)\n",
        "ref_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kgQXLCGIkTI"
      },
      "source": [
        "### Preparing the Dataset for Reinforcement Learning\n",
        "\n",
        "Reinforcement learning (RL) requires a dataset to simulate experiences and provide feedback. In our RL setup for fine-tuning a language model, we utilize a comparison dataset.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Load Dataset**:\n",
        "    - Using Huggingface's `datasets` library, we fetch the 'CarperAI/openai_summarize_comparisons' dataset's test split.\n",
        "\n",
        "2. **Filtering**:\n",
        "    - We want to ensure the prompt lengths are manageable.\n",
        "    - Filtering by word count: We retain samples where the prompt has ≤ 450 words.\n",
        "    - (Alternative Filtering by character count is commented out for reference.)\n",
        "\n",
        "3. **Shuffling and Sampling**:\n",
        "    - To ensure a diverse set of samples, we shuffle the dataset.\n",
        "    - We then select a subset (2,000 samples in this instance) for the RL process.\n",
        "\n",
        "4. **Feature Extraction**:\n",
        "    - From our shuffled dataset, we focus on the `prompt` and `chosen` fields.\n",
        "    - Rename the 'chosen' field to 'response' to align with the PPO library's requirements.\n",
        "\n",
        "5. **Dataset Conversion**:\n",
        "    - Convert the dictionary containing our features into a Huggingface Dataset format.\n",
        "\n",
        "6. **Train-Eval Split**:\n",
        "    - Split the dataset into training and evaluation subsets.\n",
        "    - Here, 80% of samples are designated for training, and the remaining 20% are for evaluation.\n",
        "\n",
        "### Outcome:\n",
        "\n",
        "By the end of this process, we will have a training dataset and an evaluation dataset ready for the RL process. These datasets will be essential in guiding the model's fine-tuning and assessing its performance during the RL loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hioROxkQIkTJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "orig_dataset = load_dataset('CarperAI/openai_summarize_comparisons', split='test')\n",
        "\n",
        "# Filter samples where the prompt length is less than or equal to 750\n",
        "filtered_dataset = orig_dataset.filter(lambda example: len(example['prompt'].split()) <= 450) # By word\n",
        "#filtered_dataset = orig_dataset.filter(lambda example: len(example['prompt']) <= 1250) # By character\n",
        "\n",
        "# Shuffle and select the first 10K samples\n",
        "#shuffled_dataset = orig_dataset.shuffle(seed=42).select(range(1000))\n",
        "shuffled_dataset = filtered_dataset.shuffle(seed=42).select(range(2000))\n",
        "\n",
        "\n",
        "# Extract the desired features.  Renaming chose to response to follow the ppo library requirements.\n",
        "new_dataset_dict = {\n",
        "    \"prompt\": shuffled_dataset[\"prompt\"],\n",
        "    \"response\": shuffled_dataset[\"chosen\"]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a new Dataset\n",
        "dataset = HFDataset.from_dict(new_dataset_dict)\n",
        "\n",
        "# Split the new_dataset into train_dataset and eval_dataset\n",
        "split_ratio = 0.8  # 80% for training, 20% for evaluation\n",
        "num_train_samples = int(split_ratio * len(dataset))\n",
        "train_dataset = dataset.select(range(num_train_samples))\n",
        "eval_dataset = dataset.select(range(num_train_samples, len(dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-EWoFfIkTJ"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0].keys())\n",
        "print(eval_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2e5BGXxIkTJ"
      },
      "source": [
        "### Tokenization of Datasets\n",
        "\n",
        "For reinforcement learning, it is crucial that the data is in a format understood by the model. This requires tokenizing our textual data into numerical tokens. Here, we'll use the tokenizer associated with our model (T5 in this case) to process our datasets.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Tokenizer Initialization**:\n",
        "    - Instantiate the tokenizer corresponding to our model (T5). If you use a different model, ensure you fetch the right tokenizer.\n",
        "\n",
        "2. **Tokenization Function**:\n",
        "    - Define a function (`tokenize_function`) that:\n",
        "        - Processes the 'prompt' in each example of the dataset.\n",
        "        - Truncates or pads the tokenized prompt to a maximum length of 512 tokens.\n",
        "        - Returns the tokenized 'input_ids' for each 'prompt' and retains the associated 'response'.\n",
        "\n",
        "3. **Apply Tokenization**:\n",
        "    - Apply the `tokenize_function` to both the training and evaluation datasets using the `map` function.\n",
        "\n",
        "### Outcome:\n",
        "\n",
        "The datasets (`train_dataset` and `eval_dataset`) are now tokenized and in a suitable format for model ingestion during the reinforcement learning loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5xBh6z9IkTP"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Instantiate your tokenizer (replace T5Tokenizer with your model's tokenizer if different)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\") # or whatever model you're using\n",
        "\n",
        "def tokenize_function(example):\n",
        "    # Tokenize the prompt and store it as input_ids. Also return the response.\n",
        "    return {\n",
        "        \"input_ids\": tokenizer(example[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=1024)[\"input_ids\"].squeeze(),\n",
        "        \"response\": example[\"response\"],\n",
        "    }\n",
        "\n",
        "# Tokenize the training and evaluation datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=False)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f9-zwbSIkTP"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTx08dafIkTQ"
      },
      "outputs": [],
      "source": [
        "# Lets check one sample of the train_dataset\n",
        "print(train_dataset[0])  # print the first example from the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCLVx3kkIkTQ"
      },
      "source": [
        "### Hyperparameter Initialization\n",
        "\n",
        "Before training the model using reinforcement learning, we need to define several hyperparameters that will guide and constrain the training process.\n",
        "\n",
        "### Data Collation:\n",
        "\n",
        "- **`collator` Function**:\n",
        "    - A helper function that takes a list of data samples and merges them into a single batch, making it suitable for processing by the model.\n",
        "    - For instance, given an input of individual key-value data samples, the function groups the values by their keys.\n",
        "\n",
        "    Example:\n",
        "    ```python\n",
        "    test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}, {\"key1\": \"value4\", \"key2\": \"value5\", \"key3\": \"value6\"}]\n",
        "    collated_data = collator(test_data)\n",
        "    ```\n",
        "\n",
        "- **Sample Data**:\n",
        "    - To visually validate the output of the `collator`, a sample is taken from the training dataset and processed.\n",
        "\n",
        "### Key Hyperparameters:\n",
        "\n",
        "- **`learning_rate`**:\n",
        "    - Controls the step size at each iteration while moving towards a minimum in the loss function. Set to `1.41e-5`.\n",
        "\n",
        "- **`max_ppo_epochs`**:\n",
        "    - Specifies the maximum number of epochs for the Proximal Policy Optimization (PPO) training. Set to `3`.\n",
        "\n",
        "- **`mini_batch_size`** & **`batch_size`**:\n",
        "    - Determines the number of samples in each mini-batch (`4`) and the overall batch size (`16`).\n",
        "\n",
        "- **`DEFAULT_REJECTED_SUMMARY_TEXT`**:\n",
        "    - A placeholder text for a bad summary. This could potentially act as a regularizer during training, though its effect needs to be verified.\n",
        "\n",
        "- **Generation Constraints** (`generation_kwargs`):\n",
        "    - `temperature`: Controls the randomness of predictions by scaling the logits before applying softmax. Set to `1.0`.\n",
        "    - `min_length`: Minimum length of the generated text. Set to `5`.\n",
        "    - `top_k` & `top_p`: Parameters controlling the nucleus sampling method. Here, `top_k` is set to `0.0` and `top_p` to `1.0`, indicating no truncation based on these parameters.\n",
        "    - `do_sample`: Boolean value determining whether to sample the outputs. Set to `True`.\n",
        "\n",
        "- **Output Length Sampling**:\n",
        "    - `output_min_length` & `output_max_length`: Define the minimum (`100`) and maximum (`400`) lengths of generated outputs.\n",
        "    - `output_length_sampler`: Samples an output length between the specified min and max values.\n",
        "\n",
        "- **`max_ppo_steps`**:\n",
        "    - Determines the total number of PPO steps during training. Set to `100`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBf3f5PkIkTQ"
      },
      "outputs": [],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}, {\"key1\": \"value4\", \"key2\": \"value5\", \"key3\": \"value6\"}]\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')\n",
        "\n",
        "# Lets sample what the collator generates:\n",
        "sample_data = [train_dataset[i] for i in range(3)]  # take first three examples\n",
        "collated_data = collator(sample_data)\n",
        "print(collated_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=1e-4\n",
        "max_ppo_epochs=5\n",
        "mini_batch_size=2\n",
        "batch_size=8"
      ],
      "metadata": {
        "id": "wJxRBHLl7L9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKn_oqTMIkTQ"
      },
      "source": [
        "### Configuration for PPO Training\n",
        "\n",
        "We leverage the `PPOConfig` from the Hugging Face `trl` library to set up the configuration required for the Proximal Policy Optimization (PPO) training.\n",
        "\n",
        "The `PPOConfig` requires and/or allows for a number of arguments that define the behavior of the PPO training loop:\n",
        "\n",
        "- **`model_name`**:\n",
        "    - Name of the model. Here, it is set as `policy_model_name`.\n",
        "\n",
        "- **`learning_rate`**:\n",
        "    - The rate at which the model adjusts based on the error during training. We've set it to the previously initialized value of `learning_rate`.\n",
        "\n",
        "- **`ppo_epochs`**:\n",
        "    - Specifies the number of epochs for PPO training. Set to the previously defined `max_ppo_epochs`.\n",
        "\n",
        "- **`mini_batch_size`**:\n",
        "    - The size of the smaller batches that the main batch is divided into, during training. Set to the previously initialized value of `mini_batch_size`.\n",
        "\n",
        "- **`batch_size`**:\n",
        "    - The number of data samples processed during each training step. We've set it to the previously initialized value of `batch_size`.\n",
        "\n",
        "For a more detailed understanding and potential additional configurations, one can refer to the [Hugging Face documentation on `trl.trainer`](https://huggingface.co/docs/trl/trainer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTi-YYjlIkTR"
      },
      "outputs": [],
      "source": [
        "# Check out https://huggingface.co/docs/trl/trainer\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=policy_model_name,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPgTETRlIkTR"
      },
      "source": [
        "### Setting Up the PPO Trainer\n",
        "\n",
        "To fine-tune the model using Proximal Policy Optimization (PPO), we use the `PPOTrainer` class from Hugging Face's `trl` library.\n",
        "\n",
        "The `PPOTrainer` class is initialized with several key arguments:\n",
        "\n",
        "- **`config`**:\n",
        "    - The configuration object created using `PPOConfig`. This contains the hyperparameters required for PPO training.\n",
        "\n",
        "- **`model`**:\n",
        "    - The model that will be fine-tuned. In this case, it is the `ppo_model` which was previously instantiated.\n",
        "\n",
        "- **`ref_model`**:\n",
        "    - The reference model, representing the model before alignment. We use `ref_model` for this purpose.\n",
        "\n",
        "- **`tokenizer`**:\n",
        "    - The tokenizer responsible for converting text into tokens suitable for the model's input. Here, it's the `policy_tokenizer` we set up before.\n",
        "\n",
        "- **`dataset`**:\n",
        "    - The training dataset. We use the tokenized `train_dataset`.\n",
        "\n",
        "- **`data_collator`**:\n",
        "    - A function to transform a list of samples to a batch. We use the `collator` function we defined earlier.\n",
        "\n",
        "This trainer will be used to conduct the PPO training loop, enabling us to fine-tune the model using reinforcement learning.\n",
        "\n",
        "For a deeper dive into the functionalities provided by the `PPOTrainer` class, one can refer to the [Hugging Face documentation on `trl.trainer`](https://huggingface.co/docs/trl/trainer).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfL_CqlTIkTR"
      },
      "outputs": [],
      "source": [
        "# Check out https://huggingface.co/docs/trl/trainer\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=policy_tokenizer,\n",
        "                         dataset=train_dataset,\n",
        "                         data_collator=collator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some initial values\n",
        "output_min_length = 128\n",
        "output_max_length = 2048\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# These hyperparams guide the generation of the completion in the policy model. We could add other params like temperature.\n",
        "generation_kwargs = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "max_ppo_steps = 256"
      ],
      "metadata": {
        "id": "4VTGWfXg7ekw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stV9MAFzIkTR"
      },
      "source": [
        "## Fine-Tuning with Reinforcement Learning\n",
        "\n",
        "Reinforcement learning offers a unique approach to fine-tuning models. The underlying principle is to allow the model to learn by receiving feedback (rewards) on its actions. In this context, an action would be generating a summary for a given text prompt.\n",
        "\n",
        "### Training Loop Overview\n",
        "\n",
        "The training loop we've crafted here follows this sequence of steps:\n",
        "\n",
        "1. **Model Prediction**: Using the policy language model (`ppo_trainer` in this case), we generate predicted summaries.\n",
        "2. **Score Generation**: We then pass these summaries to a reward model to assign a score (reward) based on the quality of the generated summary.\n",
        "3. **Model Update**: With the generated summaries and their respective scores, we use Proximal Policy Optimization (PPO) to update our policy language model.\n",
        "\n",
        "### Detailed Breakdown\n",
        "\n",
        "#### **1. Model Prediction**:\n",
        "\n",
        "- We iterate through our training data in batches (`prompt_tensors`).\n",
        "- For each prompt, we predict a summary (`summary_tensors`). This prediction is based on the generation hyperparameters we've specified (`generation_kwargs`), which guide the sampling strategy.\n",
        "\n",
        "#### **2. Score Generation**:\n",
        "\n",
        "- For each summary, we calculate a score by comparing it with a default rejected summary.\n",
        "- This step uses a separate reward model (`rm_model`), which assesses the quality of summaries.\n",
        "\n",
        "#### **3. Model Update**:\n",
        "\n",
        "- Using PPO, we update our policy model based on:\n",
        "  - The initial input (`prompt_tensors`).\n",
        "  - The generated summary (`summary_tensors`).\n",
        "  - The assigned reward (`reward_tensors`).\n",
        "  \n",
        "### Key Metrics:\n",
        "\n",
        "- `objective/kl`: Measures how different the policy's action distribution after the update is from the action distribution before the update. PPO tries to make these changes very small to avoid drastic changes.\n",
        "  \n",
        "- `ppo/returns/mean`: This is the average return achieved by the agent. Higher is better.\n",
        "\n",
        "- `ppo/policy/advantages_mean`: Measures how much better an action is than the average action at a given state. An advantage of zero means the action is just average, a positive advantage means it's better than average, and a negative one means it's worse than average.\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "- **HACK** Alert: The code contains certain hacks (like for handling variable sequence lengths) which were used to overcome specific issues during development.\n",
        "\n",
        "- **Reward Model**: The quality of the model training largely depends on the feedback it provides.\n",
        "\n",
        "### References:\n",
        "\n",
        "- [PPOTrainer in Hugging Face's TRL library](https://huggingface.co/docs/trl/trainer#trl.PPOTrainer)\n",
        "- [Using Transformer Reinforcement Learning to detoxify generative language models](https://medium.com/@ben.burtenshaw/using-transformer-reinforcement-learning-to-detoxify-generative-language-models-5198446d6786)\n",
        "- HuggingFace's example scripts in their GitHub repository.\n",
        "\n",
        "The success of reinforcement learning is deeply intertwined with the feedback mechanism and the quality of the reward signal.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "def score_summaries(full_text, summarized_text):\n",
        "\n",
        "  prompt = f\"\"\"### FULL TEXT:\\n {full_text} \\n\n",
        "  ### SUMMARIZED TEXT: \\n {summarized_text}\"\"\"\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      temperature = 0.,\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[{\"role\": \"system\", \"content\": f\"\"\"You are an expert in text summarization. Below, you are given the full text and its summarization.\n",
        "  Your role is to rate the provided summarization with scores ranging from 0 to 1, where: 0 is the lowest score, 1 is the highest score.\n",
        "  Your response should only be a double precision number that represents the scoring rate.\n",
        "  \"\"\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}],\n",
        "      request_timeout=60000\n",
        "  )\n",
        "\n",
        "  response = response['choices'][0]['message']['content']\n",
        "  score    = float(re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", response)[0])\n",
        "  return score"
      ],
      "metadata": {
        "id": "OkZ3fJM8CPnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "5q6ElT98Dzvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dataset[10000]['prompt']"
      ],
      "metadata": {
        "id": "9KrKa3f7xNZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "\n",
        "# test_string = \"Rating: 0.2\"\n",
        "# res = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", test_string)[0]\n",
        "# print(f\"The numbers list is : {res}\")"
      ],
      "metadata": {
        "id": "svS_hstV6P7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOFkiLmUIkTS"
      },
      "outputs": [],
      "source": [
        "objective_kl    = []\n",
        "returns_mean    = []\n",
        "advantages_mean = []\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for step, batch in enumerate(ppo_trainer.dataloader):\n",
        "\n",
        "    if step >= max_ppo_steps: # Break when we reach max_steps.\n",
        "        break\n",
        "\n",
        "\n",
        "    prompts = [policy_tokenizer.decode(tok) for tok in batch['input_ids']][0]\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "    # print(batch['response'])\n",
        "    # if step==0: break\n",
        "\n",
        "    if isinstance(prompt_tensors, list) and all(isinstance(item, list) for item in prompt_tensors): # HACK!!! Check if original_prompt_tensors is a list of lists\n",
        "        lengths = [len(seq) for seq in prompt_tensors] # Verify if sequences have fixed or variable length\n",
        "        unique_lengths = set(lengths)\n",
        "\n",
        "        if len(unique_lengths) > 1: # If sequences have variable lengths, pad them\n",
        "            max_length = max(unique_lengths)\n",
        "            original_prompt_tensors = [seq + [0] * (max_length - len(seq)) for seq in prompt_tensors]  # padding with zeros\n",
        "\n",
        "        prompt_tensors = [torch.tensor(seq).to(device) for seq in prompt_tensors] # Convert original_prompt_tensors to individual tensors\n",
        "\n",
        "    summary_tensors = []\n",
        "\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        prompt_tensor = torch.tensor(prompt_tensor).to(device)\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    batch[\"response\"] = [policy_tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    response = batch[\"response\"]\n",
        "\n",
        "    reward_tensors = []\n",
        "\n",
        "    for prompt, summary in zip(prompts, response):\n",
        "        score = score_summaries(prompt, response)\n",
        "        # score = float(score)\n",
        "        reward_tensors.append(torch.tensor(score))\n",
        "\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'objective/kl: {stats[\"objective/kl\"]}') # Measures how different the policy's action distribution after the update is from the action distribution before the update. PPO tries to make these changes very small to avoid sudden changes.\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}') # This is the average return achieved by the agent. Higher is better.\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}') # Measures how much better an action is than the average action at a given state.\n",
        "    print(f'STEP: {step}')\n",
        "\n",
        "    objective_kl.append(stats[\"objective/kl\"])\n",
        "    returns_mean.append(stats[\"ppo/returns/mean\"])\n",
        "    advantages_mean.append(stats[\"ppo/policy/advantages_mean\"])\n",
        "\n",
        "    print('-'.join('' for x in range(100)))\n",
        "\n",
        "end = time.time()\n",
        "print(f'TIME: {end - start}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for plotting\n",
        "t = np.array(returns_mean)\n",
        "s = range(len(returns_mean))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(s, t)\n",
        "\n",
        "ax.set(xlabel='episodes', ylabel='mean return',\n",
        "       title='Policy optimization')\n",
        "# ax.grid()\n",
        "\n",
        "fig.savefig(\"test.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cpFm4nQmtOCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yw12zuaIkTS"
      },
      "source": [
        "## Saving the Model and Tokenizer\n",
        "\n",
        "After the fine-tuning process, it's crucial to save the model's weights and the tokenizer's configuration for future use, whether it's for inference, further training, or sharing with the community.\n",
        "\n",
        "### 1. Saving the Model\n",
        "\n",
        "To preserve the state of your model post-training, use the `save_pretrained` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH2YmKQpIkTS"
      },
      "outputs": [],
      "source": [
        "# import huggingface_hub\n",
        "\n",
        "# hf_token = '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'\n",
        "\n",
        "# hf_api = huggingface_hub.HfApi(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_trainer.model.push_to_hub('PanoEvJ/T5_summarization_RLAIF', token='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')\n",
        "policy_tokenizer.push_to_hub('PanoEvJ/T5_summarization_RLAIF', token='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')"
      ],
      "metadata": {
        "id": "l6H_cP0eeaWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objective_kl"
      ],
      "metadata": {
        "id": "x4ZO9z-_IiGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "returns_mean"
      ],
      "metadata": {
        "id": "m5U5GDmwIvDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "advantages_mean"
      ],
      "metadata": {
        "id": "DyEL-Y3EIzxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1OsCHr9IkTT"
      },
      "source": [
        "## Inference using the Fine-tuned Model\n",
        "\n",
        "After saving the fine-tuned model, the next step is to utilize it for generating summaries. The model will produce outputs based on the knowledge it acquired during the RL fine-tuning process.\n",
        "\n",
        "### Loading the Model\n",
        "\n",
        "To load the model, we will use the `AutoModelForSeq2SeqLMWithValueHead` class from the `trl` library. This class is tailored for sequence-to-sequence tasks and also has the value head which was required for the Proximal Policy Optimization (PPO) algorithm:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4IaLLe3IkTT"
      },
      "outputs": [],
      "source": [
        "ppo_saved_model_path = \"PanoEvJ/T5_summarization_RLAIF\"\n",
        "\n",
        "from trl import AutoModelForSeq2SeqLMWithValueHead # https://huggingface.co/docs/trl/quickstart\n",
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(ppo_saved_model_path)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "policy_tokenizer = AutoTokenizer.from_pretrained(ppo_saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69YXl6QIkTT"
      },
      "source": [
        "### Function for Generating Summaries\n",
        "\n",
        "In order to simplify the inference process and generate summaries for new prompts, a dedicated function `generate_summary` has been defined. This function uses the trained model, its tokenizer, and other parameters to produce concise and relevant summaries for input text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd98q6UZIkTT"
      },
      "outputs": [],
      "source": [
        "def generate_summary(prompt: str, model, tokenizer, generation_kwargs, output_length_sampler) -> str:\n",
        "    \"\"\"\n",
        "    Generate a summary for a given prompt using a trained policy model.\n",
        "\n",
        "    Args:\n",
        "    - prompt (str): The input text for which a summary needs to be generated.\n",
        "    - model: The trained policy model.\n",
        "    - tokenizer: The tokenizer used for the policy model.\n",
        "    - generation_kwargs (dict): Arguments used for response generation.\n",
        "    - output_length_sampler (func): Function to sample the length of the output.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated summary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    prompt_tensor = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Ensure it's only one tensor and check its shape\n",
        "    assert prompt_tensor.dim() == 2, f\"Unexpected tensor shape: {prompt_tensor.shape}\"\n",
        "\n",
        "    # Set the generation arguments\n",
        "    max_new_tokens = output_length_sampler()\n",
        "    generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_tensor = model.generate(input_ids=prompt_tensor, **generation_kwargs)\n",
        "\n",
        "    # Decode and return the summary\n",
        "    summary = tokenizer.decode(summary_tensor[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T4IWfiPIkTU"
      },
      "outputs": [],
      "source": [
        "# text = \"SUBREDDIT: r/relationships TITLE: How do I/do I at all [20 F] tell my boyfriend [23 M] that I'm bisexual? POST: I've had two serious relationships prior to this one, both with women. They had no problem with me being bisexual and it was something known before the relationship -- my first girlfriend was also bisexual. I am now in a relationship with a guy. We've been exclusive for about a month. Having never faced this issue, I come to you, Reddit. Is this something that he needs to know? Is it really relevant to a hetero relationship, regardless of if one of the participants in the relationship is bisexual? If you guys think it is necessary, when do you think is the right time? I think my biggest fear is losing him because of it. I know that I should be with someone who is fine with who I am, but I really like the guy and I'd hate for my sexual orientation to be the thing that kills this.\"\n",
        "# text = \"SUBREDDIT: r/legaladvice TITLE: What can I do legally to restore water to my condominium!? POST: Hi, I live in SE Michigan in a condominium complex. Our water was shut off due to non-payment. (we recieved no notice) and we had to pay all that was due ($1500) We payed this yesterday at 2, they said the water would be turned on immediately. It wasn't. It's now the next day. The lady in our assosciation keeps insisting that the water meter is in another condo. Which we can't access because the person living there is never there (it's being rented) Now we're stuck with no water, no shower, no teeth brushing, no toilets, and no food for certain meals.... Please help us... What can we do? We called the police and they say that we can file a civil report for the lady not doing her job...\"\n",
        "# text = \"SUBREDDIT: r/relationships TITLE: To go or not to go? Old friend (f, 23) getting married, I (f 23) don't want to because I have to go from here in the Netherlands to USA. POST: So, I have had this friend for a long time and we have always been there for each other. But about 6 months ago I moved here to the Netherlands to be with my partner (m23). This is our first place together here and we had to buy our own furniture. Needless to say we don't really have any money for trips. My friend is getting married in March in the USA and I feel really guilty out of obligation but I really don't want to go. I don't have the money for it and I don't want to leave here and miss my partner. Reasons for not wanting to go: 1. Money 2. Missing my partner. 3. Being incredibly bored once I'm there! I won't have a car or a way to get around, so I'll just be sitting in my parents house all day. I know it's bad that I don't want to go, but I am just really dreading it. Reddit, what do I do?\"\n",
        "# text = \"SUBREDDIT: r/Advice TITLE: Bike tour around the world? POST: Hi there redditors! First of all I'd like to apologize for my English, but as you will see (I hope not), I'm not a native speaker. I'm 23-year-old who recently graduated from university and just stared my first job. Now, you see, my job is interesting and all, but it's an office job and I feel I'm not suited for this. I'm the adventures type, I want something happening around me and going to work from 9 to 6 is just killing me. The one thing that I thought of is a bike trip mostly in Europe, Asia and North Africa. The problem is that I'm from a country with an average salary around 350 euros or 450 USD. My salary is a bit higher - around 450 euros, but still not enough according to what I read is needed for such a trip, witch is about 30000 USD. My question is if somebody has done something like this without any money and if they have some tips for me. I'm thinking about sleeping outdoors or helping some locals for food and a place to crash. Is this something that could work out? I'm planning to go with my girlfriend and I think not too many people would take us in. Any help would be greatly appreciated!\"\n",
        "text = \"SUBREDDIT: r/Parenting TITLE: Question about saying 'no' to 18 month old POST: When I tell my son 'no' to something that is either dangerous (like sitting on the arm of the couch or trying to climb onto the television) or something that is an unwanted behavior (biting, hitting etc.) he looks at me and giggles before continuing to do whatever the hell he wants to do. When my husband tells him 'no' he stops what he's doing and sometimes gets upset to the point of crying (I think because his feelings are hurt). I guess the question is, how do I get him to listen to me and not just to his father? I have tried to make my voice sound louder and more masculine, but that just makes him laugh even harder.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wziQBKRrIkTU"
      },
      "outputs": [],
      "source": [
        "prompt = f\"{task_prefix}{text}\"\n",
        "generated_summary = generate_summary(prompt, ppo_model, policy_tokenizer, generation_kwargs, output_length_sampler)\n",
        "print(generated_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iULABIHEIkTU"
      },
      "source": [
        "## Conclusion and Recap\n",
        "\n",
        "In this notebook, we embarked on the ambitious journey of Reinforcement Learning from Human Feedback (RLHF) with the aim to enhance text summarization. The major components of this approach are the policy model (in this case, a T5 model) and a reward model (based on BERT). Let's recap the steps we've taken and the knowledge we've gained:\n",
        "\n",
        "1. **Loading the Policy Model (T5)**:\n",
        "   - We began by initializing the T5 model which would act as our policy model for generating text summaries.\n",
        "  \n",
        "2. **Loading the Reward Model (BERT)**:\n",
        "   - To evaluate the quality of the summaries generated by the T5 model and to give feedback, we employed a BERT-based model which was trained on a mixture of model-written summaries and human feedback.\n",
        "\n",
        "3. **Training Loop with Proximal Policy Optimization (PPO)**:\n",
        "   - For the fine-tuning of our T5 policy model, we utilized the PPO algorithm, a state-of-the-art deep reinforcement learning method.\n",
        "   - We established a loop wherein the T5 model proposed text summaries which were then evaluated by the BERT-based reward model. Using these rewards, the T5 model was fine-tuned to better align with human preferences.\n",
        "   - Throughout this loop, we monitored various metrics such as the KL divergence, mean returns, and advantages to ensure that the training was progressing desirably.\n",
        "\n",
        "4. **Inference**:\n",
        "   - After the RLHF process, we put our enhanced T5 model to the test! By employing a dedicated function, we generated summaries for new input text, reaping the rewards of our fine-tuning efforts.\n",
        "\n",
        "By leveraging the strengths of both T5 and BERT, and by harnessing the power of reinforcement learning through PPO, we aimed to create a model that produces summaries of superior quality that are more in line with human preferences.\n",
        "\n",
        "Future efforts can focus on refining the training process, experimenting with different RL algorithms, or scaling up the training data to further improve the performance.\n",
        "\n",
        "Thank you for joining on this journey, and happy summarizing!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1rRf8UUIkTU"
      },
      "source": [
        "Notebook developed by [Juan Olano](https://www.linkedin.com/in/juan-olano-b9a330112/) and [Pano Evangeliou](https://www.linkedin.com/in/p-evangeliou/) - Sept.2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym3VZ0a9IkTV"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}