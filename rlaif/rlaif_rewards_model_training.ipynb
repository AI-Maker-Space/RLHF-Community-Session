{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use an `AutoModelForSequenceClassification` as the reward model. \n",
    "\n",
    "The reward model should be trained on a dataset of paired examples, where each example is a tuple of two sequences. T\n",
    "\n",
    "he reward model should be trained to predict which example in the pair is more relevant to the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaways:\n",
    "\n",
    "1. The dataset is prepared in a special way: We create a dataset for the chosen and a dataset for the rejected. By doing this, we can calculate the forward pass of each one of these options.\n",
    "\n",
    "2. The loss function for the rewards model is calculated with a special formula: For the 2 options we have in our dataset, we use logsigmoid of the difference between the output of the forward prop of the Chosen and the Rejected options.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training\n",
    "\n",
    "The training loop implemented is carrying out a \"ranking loss\" to teach the model to distinguish between \"chosen\" and \"rejected\" samples. The idea is that we want the model to assign a higher score to the chosen samples compared to the rejected ones. \n",
    "\n",
    "#### Forward Passes\n",
    "Chosen Samples: For each batch, we do a forward pass through the model for the \"chosen\" samples (e.g., the better answers, or the answers that should be ranked higher). The model returns some scores stored in rewards_chosen.\n",
    "\n",
    "Rejected Samples: Similarly, we do another forward pass for the \"rejected\" samples (e.g., the worse answers, or the answers that should be ranked lower). The model returns some scores stored in rewards_rejected.\n",
    "\n",
    "So, for each batch, we get two sets of scores: one for the chosen and one for the rejected samples.\n",
    "\n",
    "#### Custom Loss Calculation\n",
    "Loss Calculation: The loss function aims to ensure that the score for each chosen sample is higher than the score for each corresponding rejected sample. Specifically, we want the model to maximize the difference (rewards_chosen - rewards_rejected).\n",
    "\n",
    "1. rewards_chosen - rewards_rejected: This calculates the difference between the rewards for each pair of chosen and rejected samples.\n",
    "2. logsigmoid(x): This is a smooth function that maps x to the range (0, 1). The idea is that if x is a large positive number (meaning rewards_chosen is much larger than rewards_rejected), then logsigmoid(x) approaches 0, which is what we want to minimize in a loss function.\n",
    "3. Negative sign: The negative sign is because we want to maximize this value, but most optimizers are minimizing in nature.\n",
    "4. .mean(): Finally, we take the mean over all pairs in a batch. This is our final loss value for the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import json \n",
    "import random \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context size of this model is 1024 tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# IMPORTANT: Update the padding token ID in the model configuration\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Access the config to get the context size (max_position_embeddings)\n",
    "context_size = model.config.max_position_embeddings\n",
    "print(f\"The context size of this model is {context_size} tokens.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from HF\n",
    "\n",
    "The HG dataset we expect should have: prompt, chosen, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a362624de9c4688bb0d2e5ff0874ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/juan_/.cache/huggingface/datasets/JuanKO___parquet/JuanKO--RLAIF_summarization_preference_gpt35-42b7255b666728e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd166d3eb23044dd9bbb4703cbf20ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf7011d18db4d72b7d517c061e1f016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717ca10fb4ee402c9ff0f3cd061a62a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e115be58de9f442a93ee69c536e1863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/juan_/.cache/huggingface/datasets/JuanKO___parquet/JuanKO--RLAIF_summarization_preference_gpt35-42b7255b666728e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9578542f224b839da62e507c547054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset(\"JuanKO/RLAIF_summarization_preference_gpt35\")\n",
    "\n",
    "train_dataset = dataset['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "prompts = [item['prompt'] for item in train_dataset]\n",
    "chosen = [item['chosen'] for item in train_dataset]\n",
    "rejected = [item['rejected'] for item in train_dataset]\n",
    "# Tokenization\n",
    "max_length = 512  # Choose a max_length that fits your data\n",
    "encodings = tokenizer(prompts, chosen, rejected, truncation=True, padding='max_length', max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_input_ids = []\n",
    "rejected_input_ids = []\n",
    "chosen_attention_mask = []\n",
    "rejected_attention_mask = []\n",
    "\n",
    "\n",
    "for i, prompt  in enumerate(prompts):\n",
    "    chosen_input_ids.append(tokenizer(prompts[i], chosen[i], truncation=True, padding='max_length', max_length=max_length)['input_ids'])\n",
    "    chosen_attention_mask.append(tokenizer(prompts[i], chosen[i], truncation=True, padding='max_length', max_length=max_length)['attention_mask'])\n",
    "    \n",
    "    # Assuming answer2 is the rejected answer when answer1 is chosen\n",
    "    rejected_input_ids.append(tokenizer(prompts[i], rejected[i], truncation=True, padding='max_length', max_length=max_length)['input_ids'])\n",
    "    rejected_attention_mask.append(tokenizer(prompts[i], rejected[i], truncation=True, padding='max_length', max_length=max_length)['attention_mask'])\n",
    "        \n",
    "chosen_input_ids = torch.tensor(chosen_input_ids).to(device)\n",
    "rejected_input_ids = torch.tensor(rejected_input_ids).to(device)\n",
    "chosen_attention_mask = torch.tensor(chosen_attention_mask).to(device)\n",
    "rejected_attention_mask = torch.tensor(rejected_attention_mask).to(device)\n",
    "\n",
    "dataset = TensorDataset(chosen_input_ids, chosen_attention_mask, rejected_input_ids, rejected_attention_mask)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special loss function\n",
    "For Reward Model we use a loss function based on the log of sigmoid.\n",
    "\n",
    "In this type of problem, we are less concerned with the absolute values of the outputs and more concerned with the relative difference between a \"preferred\" and a \"rejected\" output. The idea is to maximize the difference between the \"preferred\" and the \"rejected\" output so that the model learns to rank them correctly.\n",
    "\n",
    "#### Important:\n",
    "The model function is applied separately to the \"chosen\" and \"rejected\" sets of input IDs and attention masks. This implies that the \"chosen\" and \"rejected\" samples are passed separately through the model to compute their respective logits (or \"rewards\" in this context).\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "Separate Forward Passes: The model performs a forward pass for the \"chosen\" inputs (input_ids_chosen and attention_mask_chosen) and another forward pass for the \"rejected\" inputs (input_ids_rejected and attention_mask_rejected).\n",
    "\n",
    "Logits as Rewards: The outputs (rewards_chosen and rewards_rejected) are taken as the model's estimated \"rewards\" or utilities for the \"chosen\" and \"rejected\" inputs.\n",
    "\n",
    "Loss Computation: The loss is computed using these \"rewards\" via the log-sigmoid function applied to the difference between the two, as explained in previous responses.\n",
    "\n",
    "This method is very explicit about which samples are \"chosen\" and which ones are \"rejected,\" as they are processed separately.\n",
    "\n",
    "#### Note\n",
    "If we have more than 2 options (chosen, rejected), then we could implement another loss function using softmax. Something like this: \n",
    "\n",
    "    loss = -torch.log(F.softmax(output_preferred - output_rejected, dim=0) + 1e-8) # Added small constant (1e-8) to the logarithm to avoid numerical issues.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch 1/63, Loss: 0.7211203575134277\n",
      "Epoch 1/3, Batch 2/63, Loss: 0.7099162340164185\n",
      "Epoch 1/3, Batch 3/63, Loss: 0.7835023403167725\n",
      "Epoch 1/3, Batch 4/63, Loss: 0.735124945640564\n",
      "Epoch 1/3, Batch 5/63, Loss: 0.7402579188346863\n",
      "Epoch 1/3, Batch 6/63, Loss: 0.7947850823402405\n",
      "Epoch 1/3, Batch 7/63, Loss: 0.699426531791687\n",
      "Epoch 1/3, Batch 8/63, Loss: 0.6798833012580872\n",
      "Epoch 1/3, Batch 9/63, Loss: 0.8219254016876221\n",
      "Epoch 1/3, Batch 10/63, Loss: 0.6807708740234375\n",
      "Epoch 1/3, Batch 11/63, Loss: 0.7001925706863403\n",
      "Epoch 1/3, Batch 12/63, Loss: 0.6904599666595459\n",
      "Epoch 1/3, Batch 13/63, Loss: 0.7209701538085938\n",
      "Epoch 1/3, Batch 14/63, Loss: 0.7088404893875122\n",
      "Epoch 1/3, Batch 15/63, Loss: 0.6960053443908691\n",
      "Epoch 1/3, Batch 16/63, Loss: 0.6933815479278564\n",
      "Epoch 1/3, Batch 17/63, Loss: 0.6811074018478394\n",
      "Epoch 1/3, Batch 18/63, Loss: 0.6584429740905762\n",
      "Epoch 1/3, Batch 19/63, Loss: 0.6772416830062866\n",
      "Epoch 1/3, Batch 20/63, Loss: 0.6997806429862976\n",
      "Epoch 1/3, Batch 21/63, Loss: 0.7336454391479492\n",
      "Epoch 1/3, Batch 22/63, Loss: 0.6760686039924622\n",
      "Epoch 1/3, Batch 23/63, Loss: 0.6884832382202148\n",
      "Epoch 1/3, Batch 24/63, Loss: 0.6755446195602417\n",
      "Epoch 1/3, Batch 25/63, Loss: 0.7363714575767517\n",
      "Epoch 1/3, Batch 26/63, Loss: 0.713269829750061\n",
      "Epoch 1/3, Batch 27/63, Loss: 0.6638177037239075\n",
      "Epoch 1/3, Batch 28/63, Loss: 0.692337691783905\n",
      "Epoch 1/3, Batch 29/63, Loss: 0.6760097146034241\n",
      "Epoch 1/3, Batch 30/63, Loss: 0.7297677993774414\n",
      "Epoch 1/3, Batch 31/63, Loss: 0.6834999918937683\n",
      "Epoch 1/3, Batch 32/63, Loss: 0.6939586400985718\n",
      "Epoch 1/3, Batch 33/63, Loss: 0.666567325592041\n",
      "Epoch 1/3, Batch 34/63, Loss: 0.6685794591903687\n",
      "Epoch 1/3, Batch 35/63, Loss: 0.690403938293457\n",
      "Epoch 1/3, Batch 36/63, Loss: 0.696007490158081\n",
      "Epoch 1/3, Batch 37/63, Loss: 0.7099963426589966\n",
      "Epoch 1/3, Batch 38/63, Loss: 0.7525001764297485\n",
      "Epoch 1/3, Batch 39/63, Loss: 0.687143862247467\n",
      "Epoch 1/3, Batch 40/63, Loss: 0.7114355564117432\n",
      "Epoch 1/3, Batch 41/63, Loss: 0.6878182291984558\n",
      "Epoch 1/3, Batch 42/63, Loss: 0.7236620783805847\n",
      "Epoch 1/3, Batch 43/63, Loss: 0.7520896196365356\n",
      "Epoch 1/3, Batch 44/63, Loss: 0.6549840569496155\n",
      "Epoch 1/3, Batch 45/63, Loss: 0.7095823287963867\n",
      "Epoch 1/3, Batch 46/63, Loss: 0.7290489673614502\n",
      "Epoch 1/3, Batch 47/63, Loss: 0.7190009355545044\n",
      "Epoch 1/3, Batch 48/63, Loss: 0.709342360496521\n",
      "Epoch 1/3, Batch 49/63, Loss: 0.6956890821456909\n",
      "Epoch 1/3, Batch 50/63, Loss: 0.671782374382019\n",
      "Epoch 1/3, Batch 51/63, Loss: 0.6923766136169434\n",
      "Epoch 1/3, Batch 52/63, Loss: 0.6474747061729431\n",
      "Epoch 1/3, Batch 53/63, Loss: 0.6888377070426941\n",
      "Epoch 1/3, Batch 54/63, Loss: 0.7331454753875732\n",
      "Epoch 1/3, Batch 55/63, Loss: 0.632683277130127\n",
      "Epoch 1/3, Batch 56/63, Loss: 0.6793980002403259\n",
      "Epoch 1/3, Batch 57/63, Loss: 0.6704131960868835\n",
      "Epoch 1/3, Batch 58/63, Loss: 0.6711474657058716\n",
      "Epoch 1/3, Batch 59/63, Loss: 0.6953872442245483\n",
      "Epoch 1/3, Batch 60/63, Loss: 0.7055172920227051\n",
      "Epoch 1/3, Batch 61/63, Loss: 0.751813530921936\n",
      "Epoch 1/3, Batch 62/63, Loss: 0.7247276306152344\n",
      "Epoch 1/3, Batch 63/63, Loss: 0.6032596230506897\n",
      "Epoch 2/3, Batch 1/63, Loss: 0.6913726925849915\n",
      "Epoch 2/3, Batch 2/63, Loss: 0.6519124507904053\n",
      "Epoch 2/3, Batch 3/63, Loss: 0.6507434844970703\n",
      "Epoch 2/3, Batch 4/63, Loss: 0.7083494663238525\n",
      "Epoch 2/3, Batch 5/63, Loss: 0.7102222442626953\n",
      "Epoch 2/3, Batch 6/63, Loss: 0.7082144021987915\n",
      "Epoch 2/3, Batch 7/63, Loss: 0.6822233200073242\n",
      "Epoch 2/3, Batch 8/63, Loss: 0.6181206703186035\n",
      "Epoch 2/3, Batch 9/63, Loss: 0.645960807800293\n",
      "Epoch 2/3, Batch 10/63, Loss: 0.6988564133644104\n",
      "Epoch 2/3, Batch 11/63, Loss: 0.5963703989982605\n",
      "Epoch 2/3, Batch 12/63, Loss: 0.653820276260376\n",
      "Epoch 2/3, Batch 13/63, Loss: 0.6707061529159546\n",
      "Epoch 2/3, Batch 14/63, Loss: 0.6623854637145996\n",
      "Epoch 2/3, Batch 15/63, Loss: 0.6393898725509644\n",
      "Epoch 2/3, Batch 16/63, Loss: 0.6661869883537292\n",
      "Epoch 2/3, Batch 17/63, Loss: 0.7523242831230164\n",
      "Epoch 2/3, Batch 18/63, Loss: 0.6173683404922485\n",
      "Epoch 2/3, Batch 19/63, Loss: 0.6520146727561951\n",
      "Epoch 2/3, Batch 20/63, Loss: 0.6377771496772766\n",
      "Epoch 2/3, Batch 21/63, Loss: 0.6722214221954346\n",
      "Epoch 2/3, Batch 22/63, Loss: 0.6685230135917664\n",
      "Epoch 2/3, Batch 23/63, Loss: 0.6966768503189087\n",
      "Epoch 2/3, Batch 24/63, Loss: 0.6720450520515442\n",
      "Epoch 2/3, Batch 25/63, Loss: 0.6429381370544434\n",
      "Epoch 2/3, Batch 26/63, Loss: 0.6770404577255249\n",
      "Epoch 2/3, Batch 27/63, Loss: 0.7131316661834717\n",
      "Epoch 2/3, Batch 28/63, Loss: 0.6381814479827881\n",
      "Epoch 2/3, Batch 29/63, Loss: 0.6730376482009888\n",
      "Epoch 2/3, Batch 30/63, Loss: 0.6209850311279297\n",
      "Epoch 2/3, Batch 31/63, Loss: 0.7252791523933411\n",
      "Epoch 2/3, Batch 32/63, Loss: 0.6418275833129883\n",
      "Epoch 2/3, Batch 33/63, Loss: 0.6277189254760742\n",
      "Epoch 2/3, Batch 34/63, Loss: 0.6438482999801636\n",
      "Epoch 2/3, Batch 35/63, Loss: 0.726111650466919\n",
      "Epoch 2/3, Batch 36/63, Loss: 0.6524392366409302\n",
      "Epoch 2/3, Batch 37/63, Loss: 0.6079580783843994\n",
      "Epoch 2/3, Batch 38/63, Loss: 0.6470152139663696\n",
      "Epoch 2/3, Batch 39/63, Loss: 0.656032145023346\n",
      "Epoch 2/3, Batch 40/63, Loss: 0.6278740167617798\n",
      "Epoch 2/3, Batch 41/63, Loss: 0.6654036641120911\n",
      "Epoch 2/3, Batch 42/63, Loss: 0.6624702215194702\n",
      "Epoch 2/3, Batch 43/63, Loss: 0.632418155670166\n",
      "Epoch 2/3, Batch 44/63, Loss: 0.6604948043823242\n",
      "Epoch 2/3, Batch 45/63, Loss: 0.6645198464393616\n",
      "Epoch 2/3, Batch 46/63, Loss: 0.6074183583259583\n",
      "Epoch 2/3, Batch 47/63, Loss: 0.5661430358886719\n",
      "Epoch 2/3, Batch 48/63, Loss: 0.6343468427658081\n",
      "Epoch 2/3, Batch 49/63, Loss: 0.6840695738792419\n",
      "Epoch 2/3, Batch 50/63, Loss: 0.6709936857223511\n",
      "Epoch 2/3, Batch 51/63, Loss: 0.5870586037635803\n",
      "Epoch 2/3, Batch 52/63, Loss: 0.6913055181503296\n",
      "Epoch 2/3, Batch 53/63, Loss: 0.6904698014259338\n",
      "Epoch 2/3, Batch 54/63, Loss: 0.6037757396697998\n",
      "Epoch 2/3, Batch 55/63, Loss: 0.6796219348907471\n",
      "Epoch 2/3, Batch 56/63, Loss: 0.6525628566741943\n",
      "Epoch 2/3, Batch 57/63, Loss: 0.6397539973258972\n",
      "Epoch 2/3, Batch 58/63, Loss: 0.6346389651298523\n",
      "Epoch 2/3, Batch 59/63, Loss: 0.7083019614219666\n",
      "Epoch 2/3, Batch 60/63, Loss: 0.6187613010406494\n",
      "Epoch 2/3, Batch 61/63, Loss: 0.5409259796142578\n",
      "Epoch 2/3, Batch 62/63, Loss: 0.5872357487678528\n",
      "Epoch 2/3, Batch 63/63, Loss: 0.5183765292167664\n",
      "Epoch 3/3, Batch 1/63, Loss: 0.5603071451187134\n",
      "Epoch 3/3, Batch 2/63, Loss: 0.5505008101463318\n",
      "Epoch 3/3, Batch 3/63, Loss: 0.573421835899353\n",
      "Epoch 3/3, Batch 4/63, Loss: 0.5482094883918762\n",
      "Epoch 3/3, Batch 5/63, Loss: 0.556110680103302\n",
      "Epoch 3/3, Batch 6/63, Loss: 0.7239271402359009\n",
      "Epoch 3/3, Batch 7/63, Loss: 0.5223849415779114\n",
      "Epoch 3/3, Batch 8/63, Loss: 0.5515886545181274\n",
      "Epoch 3/3, Batch 9/63, Loss: 0.46814489364624023\n",
      "Epoch 3/3, Batch 10/63, Loss: 0.4173884093761444\n",
      "Epoch 3/3, Batch 11/63, Loss: 0.5285396575927734\n",
      "Epoch 3/3, Batch 12/63, Loss: 0.425374299287796\n",
      "Epoch 3/3, Batch 13/63, Loss: 0.5978004932403564\n",
      "Epoch 3/3, Batch 14/63, Loss: 0.3991837501525879\n",
      "Epoch 3/3, Batch 15/63, Loss: 0.6549056768417358\n",
      "Epoch 3/3, Batch 16/63, Loss: 0.4020735025405884\n",
      "Epoch 3/3, Batch 17/63, Loss: 0.5381050705909729\n",
      "Epoch 3/3, Batch 18/63, Loss: 0.6053138971328735\n",
      "Epoch 3/3, Batch 19/63, Loss: 0.5087023973464966\n",
      "Epoch 3/3, Batch 20/63, Loss: 0.793024480342865\n",
      "Epoch 3/3, Batch 21/63, Loss: 0.45931869745254517\n",
      "Epoch 3/3, Batch 22/63, Loss: 0.6013698577880859\n",
      "Epoch 3/3, Batch 23/63, Loss: 0.7947760820388794\n",
      "Epoch 3/3, Batch 24/63, Loss: 0.5363391041755676\n",
      "Epoch 3/3, Batch 25/63, Loss: 0.7030259370803833\n",
      "Epoch 3/3, Batch 26/63, Loss: 0.6863152980804443\n",
      "Epoch 3/3, Batch 27/63, Loss: 0.4905928373336792\n",
      "Epoch 3/3, Batch 28/63, Loss: 0.684872031211853\n",
      "Epoch 3/3, Batch 29/63, Loss: 0.6442661285400391\n",
      "Epoch 3/3, Batch 30/63, Loss: 0.6444877982139587\n",
      "Epoch 3/3, Batch 31/63, Loss: 0.5593566298484802\n",
      "Epoch 3/3, Batch 32/63, Loss: 0.7649823427200317\n",
      "Epoch 3/3, Batch 33/63, Loss: 0.5118940472602844\n",
      "Epoch 3/3, Batch 34/63, Loss: 0.4611181616783142\n",
      "Epoch 3/3, Batch 35/63, Loss: 0.507219672203064\n",
      "Epoch 3/3, Batch 36/63, Loss: 0.5270286202430725\n",
      "Epoch 3/3, Batch 37/63, Loss: 0.5243211984634399\n",
      "Epoch 3/3, Batch 38/63, Loss: 0.5440203547477722\n",
      "Epoch 3/3, Batch 39/63, Loss: 0.6228348016738892\n",
      "Epoch 3/3, Batch 40/63, Loss: 0.6647980213165283\n",
      "Epoch 3/3, Batch 41/63, Loss: 0.6893214583396912\n",
      "Epoch 3/3, Batch 42/63, Loss: 0.4420173764228821\n",
      "Epoch 3/3, Batch 43/63, Loss: 0.6178452372550964\n",
      "Epoch 3/3, Batch 44/63, Loss: 0.4707624912261963\n",
      "Epoch 3/3, Batch 45/63, Loss: 0.7402176856994629\n",
      "Epoch 3/3, Batch 46/63, Loss: 0.5128142833709717\n",
      "Epoch 3/3, Batch 47/63, Loss: 0.45942193269729614\n",
      "Epoch 3/3, Batch 48/63, Loss: 0.6441348195075989\n",
      "Epoch 3/3, Batch 49/63, Loss: 0.5159709453582764\n",
      "Epoch 3/3, Batch 50/63, Loss: 0.6945861577987671\n",
      "Epoch 3/3, Batch 51/63, Loss: 0.5859490633010864\n",
      "Epoch 3/3, Batch 52/63, Loss: 0.5205870270729065\n",
      "Epoch 3/3, Batch 53/63, Loss: 0.5632802248001099\n",
      "Epoch 3/3, Batch 54/63, Loss: 0.5808659195899963\n",
      "Epoch 3/3, Batch 55/63, Loss: 0.5224118232727051\n",
      "Epoch 3/3, Batch 56/63, Loss: 0.5044605135917664\n",
      "Epoch 3/3, Batch 57/63, Loss: 0.5099865198135376\n",
      "Epoch 3/3, Batch 58/63, Loss: 0.7161719799041748\n",
      "Epoch 3/3, Batch 59/63, Loss: 0.6162353754043579\n",
      "Epoch 3/3, Batch 60/63, Loss: 0.5065014362335205\n",
      "Epoch 3/3, Batch 61/63, Loss: 0.6625232696533203\n",
      "Epoch 3/3, Batch 62/63, Loss: 0.7188190221786499\n",
      "Epoch 3/3, Batch 63/63, Loss: 0.5505347847938538\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        \n",
    "        chosen_input_ids, chosen_attention_mask, rejected_input_ids, rejected_attention_mask = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "       # Forward pass for the \"chosen\" samples\n",
    "        rewards_chosen = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask)[0]\n",
    "        \n",
    "        # Forward pass for the \"rejected\" samples\n",
    "        rewards_rejected = model(input_ids=rejected_input_ids, attention_mask=rejected_attention_mask)[0]\n",
    "        \n",
    "        # Compute the custom loss\n",
    "        # Here's how this loss function works:\n",
    "        # 1. rewards_chosen and rewards_rejected are the output scores from the model for the \"chosen\" and \"rejected\" summaries, respectively.\n",
    "        # 2. rewards_chosen - rewards_rejected computes the difference between these two rewards. If the model is correctly ranking the summaries, this difference should be positive (i.e., rewards_chosen should be greater than rewards_rejected).\n",
    "        # 3. nn.functional.logsigmoid(x) computes the log-sigmoid of x. The log-sigmoid function maps its input to a range between negative infinity and zero. For positive inputs, the output is closer to zero, and for negative inputs, the output is a large negative number.\n",
    "        # 4. By minimizing the negative log-sigmoid of the difference, the model is encouraged to make rewards_chosen - rewards_rejected as large as possible, thereby pushing rewards_chosen to be higher than rewards_rejected.\n",
    "        # 5. Thus, although the model is not explicitly told which summary is \"chosen\" and which one is \"rejected,\" it learns to associate higher scores with \"chosen\" summaries and lower scores with \"rejected\" summaries by minimizing this custom loss function.             \n",
    "        loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(loader)}, Loss: {loss.item()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rlaif_rewards_model\\\\tokenizer_config.json',\n",
       " 'rlaif_rewards_model\\\\special_tokens_map.json',\n",
       " 'rlaif_rewards_model\\\\vocab.json',\n",
       " 'rlaif_rewards_model\\\\merges.txt',\n",
       " 'rlaif_rewards_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a directory\n",
    "save_directory = \"rlaif_rewards_model\"\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Optionally, save the tokenizer as well, especially if you've added special tokens or made other changes\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_reward(model, tokenizer, prompt, answer1, answer2):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, [answer1, answer2], return_tensors='pt', padding=True, truncation=True, max_length=100)\n",
    "    \n",
    "    model.to(device)\n",
    "    inputs.to(device)\n",
    "\n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Interpret the result \n",
    "    if probs[0, 0] > probs[0, 1]:\n",
    "        print(f\"The model prefers '{answer1}' with a probability of {probs[0, 0]:.4f}\")\n",
    "    else:\n",
    "        print(f\"The model prefers '{answer2}' with a probability of {probs[0, 1]:.4f}\")\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model prefers 'AI is making strides in healthcare for diagnosis, and reinforcement learning is advancing robotics.' with a probability of 0.5771\n",
      "tensor([[3.1124, 3.4233]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "prompt = \"What are the latest developments in artificial intelligence?\"\n",
    "answer1 = \"GANs are revolutionizing image creation, and NLP models like GPT-3 are transforming language tasks.\"\n",
    "answer2 = \"AI is making strides in healthcare for diagnosis, and reinforcement learning is advancing robotics.\"\n",
    "\n",
    "logits = calc_reward(model, tokenizer, prompt, answer1, answer2)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model prefers 'And how has your government done that?\n",
      "\n",
      "Ludwig von Mises\n",
      "\n",
      "From the top down, the economy has become much better than it has been in the past several years.' with a probability of 0.5764\n",
      "tensor([[3.1492, 3.4571]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the current state of the economy?\"\n",
    "answer1 = \"I'm seeing some of the data back on here, about how much we need to increase our business expenditures. In a recent report, the Congressional Budget Office's Bureau of Economic Analysis estimated that the\"\n",
    "answer2 = \"And how has your government done that?\\n\\nLudwig von Mises\\n\\nFrom the top down, the economy has become much better than it has been in the past several years.\"\n",
    "\n",
    "logits = calc_reward(model, tokenizer, prompt, answer1, answer2)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "hf_token = getpass.getpass(\"Enter your HUGGINGFACE TOKEN: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_hub = \"ENTER YOUR HUGGINGFACE MODEL REPO\"\n",
    "model.push_to_hub(hf_hub, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
