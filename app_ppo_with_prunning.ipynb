{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune T5-Base with Reinforcement Learning (PPO) and PEFT to Generate better Summaries\n",
    "\n",
    "Useful references: \n",
    "\n",
    "https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "\n",
    "https://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-ppo-deeplearning-ai\n",
    "\n",
    "https://github.com/huggingface/trl/blob/main/tests/test_ppo_trainer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward model: ideally a SequenceClassification type of model: We will use Bert\n",
    "\n",
    "Policy model: ideally a Seq2SeqLM: We will use T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dg9dg46s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-brook-1</strong> at: <a href='https://wandb.ai/jcolano/rlhf_ppo_v1/runs/dg9dg46s' target=\"_blank\">https://wandb.ai/jcolano/rlhf_ppo_v1/runs/dg9dg46s</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230910_142052-dg9dg46s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dg9dg46s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae34fc06698e41dfa220c83776dfaee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888886984852, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dropbox\\Desktop\\MAchineLearning\\rlhf\\experiment01\\wandb\\run-20230910_144502-lpqp4ndg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcolano/rlhf_ppo_v1/runs/lpqp4ndg' target=\"_blank\">bright-cloud-2</a></strong> to <a href='https://wandb.ai/jcolano/rlhf_ppo_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcolano/rlhf_ppo_v1' target=\"_blank\">https://wandb.ai/jcolano/rlhf_ppo_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcolano/rlhf_ppo_v1/runs/lpqp4ndg' target=\"_blank\">https://wandb.ai/jcolano/rlhf_ppo_v1/runs/lpqp4ndg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcolano/rlhf_ppo_v1/runs/lpqp4ndg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1d544989590>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"rlhf_ppo_v1\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "\n",
    "from peft import PeftModel, PeftConfig,  TaskType\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "# AutoModelForCausalLMWithValueHead & AutoModelForSeq2SeqLMWithValueHead: A transformer model with an additional scalar output for each token which can be used as a value function in reinforcement learning.\n",
    "# https://huggingface.co/docs/trl/models#trl.AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "import trl \n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead # https://huggingface.co/docs/trl/quickstart\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model_bert_hf_experiment2/ were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'classifier.original_module.bias', 'bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'classifier.original_module.weight', 'bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'classifier.modules_to_save.default.bias', 'bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'classifier.modules_to_save.default.weight', 'bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.11.attention.self.query.lora_B.default.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model_bert_hf_experiment2/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the directory where you saved the model and tokenizer\n",
    "reward_model_directory = \"model_bert_hf_experiment2\"\n",
    "\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\"./model_bert_hf_experiment2/\")\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"./model_bert_hf_experiment2/\")\n",
    "rm_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def score_summaries(model, tokenizer, chosen_summary, rejected_summary):\n",
    "    # Tokenize the inputs\n",
    "    chosen_tokens = tokenizer(chosen_summary, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "    rejected_tokens = tokenizer(rejected_summary, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    chosen_tokens.to(device)\n",
    "    rejected_tokens.to(device)\n",
    "    \n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        chosen_logits = model(**chosen_tokens).logits\n",
    "        rejected_logits = model(**rejected_tokens).logits\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    chosen_probs = F.softmax(chosen_logits, dim=-1)\n",
    "    rejected_probs = F.softmax(rejected_logits, dim=-1)\n",
    "\n",
    "    # Assuming the positive class (indicating 'chosen' is good) is the second one\n",
    "    chosen_score = chosen_probs[0][1].item()\n",
    "    rejected_score = rejected_probs[0][1].item()\n",
    "    \n",
    "    # Extract logits for each summary\n",
    "    chosen_logit = chosen_logits[0][1].item()\n",
    "    rejected_logit = rejected_logits[0][1].item()\n",
    "\n",
    "    return chosen_score, rejected_score, chosen_logit, rejected_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chosen_summary = \"Water meter in another condo is not in our condo. What can we do legally to restore water to my condo complex?\"\n",
    "rejected_summary = \"Go fix the problem.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Score: 0.6342\n",
      "Rejected Score: 0.5455\n",
      "Chosen Logit: 0.1866\n",
      "Rejected Logit: 0.1093\n"
     ]
    }
   ],
   "source": [
    "chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(rm_model, rm_tokenizer, chosen_summary, rejected_summary)\n",
    "\n",
    "print(f\"Chosen Score: {chosen_score:.4f}\")\n",
    "print(f\"Rejected Score: {rejected_score:.4f}\")\n",
    "\n",
    "print(f\"Chosen Logit: {chosen_logit:.4f}\")\n",
    "print(f\"Rejected Logit: {rejected_logit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the T5\n",
    "\n",
    "This is the model that we will try to fine-tune with RLHF\n",
    "\n",
    "Lets load it and test some inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_path = \"./model_base_t5_finetuned\"\n",
    "policy_model_name = \"t5-base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = T5ForConditionalGeneration.from_pretrained(policy_model_path)\n",
    "policy_model.to(device)\n",
    "policy_tokenizer = T5Tokenizer.from_pretrained(policy_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL;DR: I'm bisexual and I'm in a hetero relationship. Is it necessary to tell my boyfriend that I'm bisexual? When do you think is the right time?\n",
      "Chosen Score: 0.5943\n",
      "Rejected Score: 0.5193\n",
      "Chosen Logit: 0.0889\n",
      "Rejected Logit: 0.2162\n"
     ]
    }
   ],
   "source": [
    "task_prefix = \"summarize: \" \n",
    "\n",
    "text = \"SUBREDDIT: r/relationships TITLE: How do I/do I at all [20 F] tell my boyfriend [23 M] that I'm bisexual? POST: I've had two serious relationships prior to this one, both with women. They had no problem with me being bisexual and it was something known before the relationship -- my first girlfriend was also bisexual. I am now in a relationship with a guy. We've been exclusive for about a month. Having never faced this issue, I come to you, Reddit. Is this something that he needs to know? Is it really relevant to a hetero relationship, regardless of if one of the participants in the relationship is bisexual? If you guys think it is necessary, when do you think is the right time? I think my biggest fear is losing him because of it. I know that I should be with someone who is fine with who I am, but I really like the guy and I'd hate for my sexual orientation to be the thing that kills this.\"\n",
    "#text = \"SUBREDDIT: r/legaladvice TITLE: What can I do legally to restore water to my condominium!? POST: Hi, I live in SE Michigan in a condominium complex. Our water was shut off due to non-payment. (we recieved no notice) and we had to pay all that was due ($1500) We payed this yesterday at 2, they said the water would be turned on immediately. It wasn't. It's now the next day. The lady in our assosciation keeps insisting that the water meter is in another condo. Which we can't access because the person living there is never there (it's being rented) Now we're stuck with no water, no shower, no teeth brushing, no toilets, and no food for certain meals.... Please help us... What can we do? We called the police and they say that we can file a civil report for the lady not doing her job...\"\n",
    "prompt = f\"{task_prefix}{text}\"\n",
    "input_ids = policy_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = policy_model.generate(input_ids, max_length=100).to(device)\n",
    "\n",
    "strOutput = policy_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(strOutput)\n",
    "\n",
    "chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(rm_model, rm_tokenizer, strOutput, \"\")\n",
    "\n",
    "print(f\"Chosen Score: {chosen_score:.4f}\")\n",
    "print(f\"Rejected Score: {rejected_score:.4f}\")\n",
    "\n",
    "print(f\"Chosen Logit: {chosen_logit:.4f}\")\n",
    "print(f\"Rejected Logit: {rejected_logit:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.10,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_peft_model = get_peft_model(policy_model, lora_config)\n",
    "policy_peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884736 || all params: 223788288 || trainable%: 0.3953450861557152\n"
     ]
    }
   ],
   "source": [
    "policy_peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate ppo_model passing the policy_peft_model from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForSeq2SeqLMWithValueHead(\n",
       "  (pretrained_model): PeftModelForSeq2SeqLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): T5ForConditionalGeneration(\n",
       "        (shared): Embedding(32128, 768)\n",
       "        (encoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (decoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/trl/quickstart\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(policy_peft_model,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "ppo_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the reference model.  The reference model will represent the LLM before allignment. \n",
    "\n",
    "I use \"create_reference_model\", a function defined by the $AutoModelForSeq2SeqLMWithValueHead$ class of Huggingface.\n",
    "\n",
    "https://huggingface.co/docs/trl/models#trl.create_reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(\n",
       "                in_features=768, out_features=768, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_model = create_reference_model(policy_model)\n",
    "ref_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset that we will use for the RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/juan_/.cache/huggingface/datasets/CarperAI___parquet/CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at C:\\Users\\juan_\\.cache\\huggingface\\datasets\\CarperAI___parquet\\CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-d5c2170aaeb9b06c.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\juan_\\.cache\\huggingface\\datasets\\CarperAI___parquet\\CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-f81caef5de9ecb07.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "orig_dataset = load_dataset('CarperAI/openai_summarize_comparisons', split='test')\n",
    "\n",
    "# Filter samples where the prompt length is less than or equal to 750\n",
    "filtered_dataset = orig_dataset.filter(lambda example: len(example['prompt'].split()) <= 450) # By word\n",
    "#filtered_dataset = orig_dataset.filter(lambda example: len(example['prompt']) <= 1250) # By character\n",
    "\n",
    "# Shuffle and select the first 10K samples\n",
    "#shuffled_dataset = orig_dataset.shuffle(seed=42).select(range(1000))\n",
    "shuffled_dataset = filtered_dataset.shuffle(seed=42).select(range(2000)) \n",
    "\n",
    "\n",
    "# Extract the desired features.  Renaming chose to response to follow the ppo library requirements.\n",
    "new_dataset_dict = {\n",
    "    \"prompt\": shuffled_dataset[\"prompt\"],\n",
    "    \"response\": shuffled_dataset[\"chosen\"]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a new Dataset\n",
    "dataset = HFDataset.from_dict(new_dataset_dict)\n",
    "\n",
    "# Split the new_dataset into train_dataset and eval_dataset\n",
    "split_ratio = 0.8  # 80% for training, 20% for evaluation\n",
    "num_train_samples = int(split_ratio * len(dataset))\n",
    "train_dataset = dataset.select(range(num_train_samples))\n",
    "eval_dataset = dataset.select(range(num_train_samples, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'response'])\n",
      "dict_keys(['prompt', 'response'])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0].keys())\n",
    "print(eval_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031aaf8ea63e4da3a008449676bebc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850b9646c93a47de90eb9cdc162d7b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Instantiate your tokenizer (replace T5Tokenizer with your model's tokenizer if different)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\") # or whatever model you're using\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # Tokenize the prompt and store it as input_ids. Also return the response.\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(example[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=512)[\"input_ids\"].squeeze(),\n",
    "        \"response\": example[\"response\"],\n",
    "    }\n",
    "\n",
    "# Tokenize the training and evaluation datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=False)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'input_ids'],\n",
       "    num_rows: 1600\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"SUBREDDIT: r/relationship_advice\\nTITLE: [20/m] My girlfriend [20/f] has become very distant and weird\\nPOST: I have been in a relationship with my girlfriend for a little bit over 1 year. We recently had a breakup because I was distant and she thought I was cheating on her (which I wasn't). Before the breakup, she wanted to spend as much time with me as she could, but recently she has been very distant. We used to go to eachothers places overnight almost daily, but nowadays she does not want to come over to my place or want me to go over to hers (We both live on our own). She also used to talk to me all the time on facebook, but now she pretty much only replies to what I talk, and does not try to keep the conversation going. She has became pretty slow at replying, but when I'm with her, she replies instantly to her other friends who text her. \\n\\nI'm really lost at this situation, because I feel like she does not want to be with me anymore. I know that she's taking SSRI medications at the moment for anxiety. But she was taking them before we broke up for couple weeks. Her explanation for this situation is that she wants to spend some time alone, because she is working so much and never has time to be alone. This bothers me because she used to want to be with me all the time she could, and now she does not even try to schedule time for me.\", 'response': 'TL;DR:  gf has became very distant after we got back together, even though before we  broke up she was really into me.', 'input_ids': [3, 4138, 25582, 11253, 3177, 10, 3, 52, 87, 60, 6105, 2009, 834, 9, 26, 7287, 15, 332, 3177, 3765, 10, 784, 1755, 87, 51, 908, 499, 17442, 784, 1755, 87, 89, 908, 65, 582, 182, 10382, 11, 10088, 276, 14464, 10, 27, 43, 118, 16, 3, 9, 1675, 28, 82, 17442, 21, 3, 9, 385, 720, 147, 209, 215, 5, 101, 1310, 141, 3, 9, 1733, 413, 250, 27, 47, 10382, 11, 255, 816, 27, 47, 15009, 53, 30, 160, 41, 3339, 27, 2088, 31, 17, 137, 3103, 8, 1733, 413, 6, 255, 1114, 12, 1492, 38, 231, 97, 28, 140, 38, 255, 228, 6, 68, 1310, 255, 65, 118, 182, 10382, 5, 101, 261, 12, 281, 12, 284, 9269, 7, 1747, 8521, 966, 1444, 6, 68, 18811, 255, 405, 59, 241, 12, 369, 147, 12, 82, 286, 42, 241, 140, 12, 281, 147, 12, 160, 7, 41, 1326, 321, 619, 30, 69, 293, 137, 451, 92, 261, 12, 1350, 12, 140, 66, 8, 97, 30, 13301, 6, 68, 230, 255, 1134, 231, 163, 26719, 12, 125, 27, 1350, 6, 11, 405, 59, 653, 12, 453, 8, 3634, 352, 5, 451, 65, 1632, 1134, 2684, 44, 8776, 53, 6, 68, 116, 27, 31, 51, 28, 160, 6, 255, 26719, 8361, 12, 160, 119, 803, 113, 1499, 160, 5, 27, 31, 51, 310, 1513, 44, 48, 1419, 6, 250, 27, 473, 114, 255, 405, 59, 241, 12, 36, 28, 140, 7595, 5, 27, 214, 24, 255, 31, 7, 838, 3, 4256, 5593, 11208, 44, 8, 798, 21, 6261, 5, 299, 255, 47, 838, 135, 274, 62, 8238, 95, 21, 1158, 1274, 5, 1347, 7295, 21, 48, 1419, 19, 24, 255, 2746, 12, 1492, 128, 97, 2238, 6, 250, 255, 19, 464, 78, 231, 11, 470, 65, 97, 12, 36, 2238, 5, 100, 13965, 7, 140, 250, 255, 261, 12, 241, 12, 36, 28, 140, 66, 8, 97, 255, 228, 6, 11, 230, 255, 405, 59, 237, 653, 12, 2023, 97, 21, 140, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Lets check one sample of the train_dataset\n",
    "print(train_dataset[0])  # print the first example from the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}, {'key1': 'value4', 'key2': 'value5', 'key3': 'value6'}]\n",
      "Collator output: {'key1': ['value1', 'value4'], 'key2': ['value2', 'value5'], 'key3': ['value3', 'value6']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}, {\"key1\": \"value4\", \"key2\": \"value5\", \"key3\": \"value6\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'response', 'input_ids'])\n"
     ]
    }
   ],
   "source": [
    "# Lets sample what the collator generates:\n",
    "sample_data = [train_dataset[i] for i in range(3)]  # take first three examples\n",
    "collated_data = collator(sample_data)\n",
    "print(collated_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=3\n",
    "mini_batch_size=4\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out https://huggingface.co/docs/trl/trainer\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=policy_model_name,    \n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out https://huggingface.co/docs/trl/trainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=policy_tokenizer, \n",
    "                         dataset=train_dataset, \n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning loop consists of the following main steps:\n",
    "1. Get the query responses from the policy LLM (PEFT model).\n",
    "2. Get reward from the Rewards model\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\juan_\\AppData\\Local\\Temp\\ipykernel_12104\\415800031.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prompt_tensor = torch.tensor(prompt_tensor).to(device)\n",
      "1it [00:16, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0\n",
      "ppo/returns/mean: 0.3847939372062683\n",
      "ppo/policy/advantages_mean: 0.00486169196665287\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:34, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.005069880746304989\n",
      "ppo/returns/mean: 0.38824570178985596\n",
      "ppo/policy/advantages_mean: 0.001255544601008296\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:53, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.027740254998207092\n",
      "ppo/returns/mean: 0.4024064540863037\n",
      "ppo/policy/advantages_mean: 0.005844333209097385\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:08, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.008470938540995121\n",
      "ppo/returns/mean: 0.4360160231590271\n",
      "ppo/policy/advantages_mean: 0.008006769232451916\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:26, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.0028168456628918648\n",
      "ppo/returns/mean: 0.4166437089443207\n",
      "ppo/policy/advantages_mean: 0.0039040702395141125\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:43, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.015527371317148209\n",
      "ppo/returns/mean: 0.4231906831264496\n",
      "ppo/policy/advantages_mean: 0.0028870929963886738\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [02:00, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.0020126374438405037\n",
      "ppo/returns/mean: 0.4269492030143738\n",
      "ppo/policy/advantages_mean: 0.0030096229165792465\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:20, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.06836467236280441\n",
      "ppo/returns/mean: 0.41612517833709717\n",
      "ppo/policy/advantages_mean: 0.0038263590540736914\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:37, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.08298622071743011\n",
      "ppo/returns/mean: 0.44846484065055847\n",
      "ppo/policy/advantages_mean: 0.0022622086107730865\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:53, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.005825418047606945\n",
      "ppo/returns/mean: 0.46129149198532104\n",
      "ppo/policy/advantages_mean: 0.003759412094950676\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:11, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.005642858799546957\n",
      "ppo/returns/mean: 0.4399201571941376\n",
      "ppo/policy/advantages_mean: 0.0013956755865365267\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [03:27, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.005834147334098816\n",
      "ppo/returns/mean: 0.4514174461364746\n",
      "ppo/policy/advantages_mean: 0.003677933244034648\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [03:44, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.011828919872641563\n",
      "ppo/returns/mean: 0.4530404210090637\n",
      "ppo/policy/advantages_mean: 0.0011673825792968273\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [04:01, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.033650025725364685\n",
      "ppo/returns/mean: 0.4805707633495331\n",
      "ppo/policy/advantages_mean: 0.004944793879985809\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [04:19, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.04556742310523987\n",
      "ppo/returns/mean: 0.4878147542476654\n",
      "ppo/policy/advantages_mean: 0.00022911513224244118\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [04:36, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.052357759326696396\n",
      "ppo/returns/mean: 0.47049078345298767\n",
      "ppo/policy/advantages_mean: 0.0005096103996038437\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [04:54, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.009967220947146416\n",
      "ppo/returns/mean: 0.4759097099304199\n",
      "ppo/policy/advantages_mean: -0.00103578413836658\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [05:12, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.02261665277183056\n",
      "ppo/returns/mean: 0.4794938266277313\n",
      "ppo/policy/advantages_mean: 0.004009743221104145\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [05:31, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0016215275973081589\n",
      "ppo/returns/mean: 0.4834381937980652\n",
      "ppo/policy/advantages_mean: 0.002844936214387417\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [05:51, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.014883600175380707\n",
      "ppo/returns/mean: 0.4874545931816101\n",
      "ppo/policy/advantages_mean: 0.001672036712989211\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [06:11, 18.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.038705311715602875\n",
      "ppo/returns/mean: 0.48216789960861206\n",
      "ppo/policy/advantages_mean: 0.005550017114728689\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [06:27, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.0019519738852977753\n",
      "ppo/returns/mean: 0.4977434575557709\n",
      "ppo/policy/advantages_mean: 0.002608383074402809\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [06:46, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.015746327117085457\n",
      "ppo/returns/mean: 0.4924009442329407\n",
      "ppo/policy/advantages_mean: -0.0029126708395779133\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [07:03, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.13578671216964722\n",
      "ppo/returns/mean: 0.49223825335502625\n",
      "ppo/policy/advantages_mean: -0.0014680366730317473\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [07:18, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.18424507975578308\n",
      "ppo/returns/mean: 0.5160964727401733\n",
      "ppo/policy/advantages_mean: 0.0035873521119356155\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [07:36, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.07853008806705475\n",
      "ppo/returns/mean: 0.5125787854194641\n",
      "ppo/policy/advantages_mean: 0.0018142350018024445\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [07:54, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.1536068469285965\n",
      "ppo/returns/mean: 0.5129472017288208\n",
      "ppo/policy/advantages_mean: -0.0031952625140547752\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [08:13, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.02651580050587654\n",
      "ppo/returns/mean: 0.5035720467567444\n",
      "ppo/policy/advantages_mean: 0.0005308630643412471\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [08:33, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.04060036689043045\n",
      "ppo/returns/mean: 0.5135884284973145\n",
      "ppo/policy/advantages_mean: 0.002005363814532757\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [08:52, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.06144071742892265\n",
      "ppo/returns/mean: 0.5074781179428101\n",
      "ppo/policy/advantages_mean: 0.006083873566240072\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [09:09, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.1026630848646164\n",
      "ppo/returns/mean: 0.497572124004364\n",
      "ppo/policy/advantages_mean: -0.004282218404114246\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [09:24, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.11476826667785645\n",
      "ppo/returns/mean: 0.5155744552612305\n",
      "ppo/policy/advantages_mean: -0.0009203864028677344\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [09:41, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.018813099712133408\n",
      "ppo/returns/mean: 0.5048032999038696\n",
      "ppo/policy/advantages_mean: 0.0009756293147802353\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [09:59, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.004020830616354942\n",
      "ppo/returns/mean: 0.50152587890625\n",
      "ppo/policy/advantages_mean: -0.0017703983467072248\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [10:16, 17.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.028400693088769913\n",
      "ppo/returns/mean: 0.5098116397857666\n",
      "ppo/policy/advantages_mean: -0.00064779695821926\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [10:35, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.004725713282823563\n",
      "ppo/returns/mean: 0.5155090093612671\n",
      "ppo/policy/advantages_mean: 0.005265518091619015\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [10:53, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.03030284121632576\n",
      "ppo/returns/mean: 0.5224802494049072\n",
      "ppo/policy/advantages_mean: 0.000699183321557939\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [11:12, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0028322823345661163\n",
      "ppo/returns/mean: 0.5142924785614014\n",
      "ppo/policy/advantages_mean: 0.00593021884560585\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [11:30, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.06499931961297989\n",
      "ppo/returns/mean: 0.5349175930023193\n",
      "ppo/policy/advantages_mean: -0.0069395094178617\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [11:50, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.015764307230710983\n",
      "ppo/returns/mean: 0.5187349915504456\n",
      "ppo/policy/advantages_mean: -0.0023731673136353493\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [12:07, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.018244529142975807\n",
      "ppo/returns/mean: 0.5299811363220215\n",
      "ppo/policy/advantages_mean: -0.00019256211817264557\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [12:23, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.058882132172584534\n",
      "ppo/returns/mean: 0.5251530408859253\n",
      "ppo/policy/advantages_mean: -0.0028166293632239103\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [12:42, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.01506706140935421\n",
      "ppo/returns/mean: 0.5216745138168335\n",
      "ppo/policy/advantages_mean: -0.007766125723719597\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [13:00, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0036642001941800117\n",
      "ppo/returns/mean: 0.5320895910263062\n",
      "ppo/policy/advantages_mean: -0.000767915858887136\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [13:18, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.03506341576576233\n",
      "ppo/returns/mean: 0.5339188575744629\n",
      "ppo/policy/advantages_mean: 0.010133009403944016\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [13:37, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.03634041175246239\n",
      "ppo/returns/mean: 0.5373767018318176\n",
      "ppo/policy/advantages_mean: -0.0011674811830744147\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [13:55, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0006244629621505737\n",
      "ppo/returns/mean: 0.5391950011253357\n",
      "ppo/policy/advantages_mean: -0.008310972712934017\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [14:11, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.09847931563854218\n",
      "ppo/returns/mean: 0.5256244540214539\n",
      "ppo/policy/advantages_mean: -0.004047184716910124\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [14:27, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.007130159065127373\n",
      "ppo/returns/mean: 0.5404835939407349\n",
      "ppo/policy/advantages_mean: -0.0006340897525660694\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [14:44, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.09234826266765594\n",
      "ppo/returns/mean: 0.5280537605285645\n",
      "ppo/policy/advantages_mean: 0.00522202393040061\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [15:01, 17.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.011652393266558647\n",
      "ppo/returns/mean: 0.5378813743591309\n",
      "ppo/policy/advantages_mean: 0.006647405680269003\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [15:18, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.010729705914855003\n",
      "ppo/returns/mean: 0.5417889356613159\n",
      "ppo/policy/advantages_mean: -0.0059839775785803795\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [15:36, 17.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0522296279668808\n",
      "ppo/returns/mean: 0.5455126166343689\n",
      "ppo/policy/advantages_mean: 0.003960065543651581\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [15:53, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.07429303228855133\n",
      "ppo/returns/mean: 0.5376936793327332\n",
      "ppo/policy/advantages_mean: 0.003455133643001318\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [16:09, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.015849264338612556\n",
      "ppo/returns/mean: 0.5331861972808838\n",
      "ppo/policy/advantages_mean: -0.0017464521806687117\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [16:26, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.10449449717998505\n",
      "ppo/returns/mean: 0.5360128879547119\n",
      "ppo/policy/advantages_mean: 0.0010660793632268906\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [16:43, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.018445100635290146\n",
      "ppo/returns/mean: 0.5541489124298096\n",
      "ppo/policy/advantages_mean: 0.00361231598071754\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [16:59, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.13619235157966614\n",
      "ppo/returns/mean: 0.5405611991882324\n",
      "ppo/policy/advantages_mean: 0.002603059634566307\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [17:16, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.054558731615543365\n",
      "ppo/returns/mean: 0.5453513860702515\n",
      "ppo/policy/advantages_mean: -0.0017076923977583647\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [17:34, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.10614021122455597\n",
      "ppo/returns/mean: 0.5431622266769409\n",
      "ppo/policy/advantages_mean: 0.002949802204966545\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [17:52, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.008595196530222893\n",
      "ppo/returns/mean: 0.5485168695449829\n",
      "ppo/policy/advantages_mean: 0.0012846844037994742\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [18:10, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.06707711517810822\n",
      "ppo/returns/mean: 0.5445464849472046\n",
      "ppo/policy/advantages_mean: 0.0017537561943754554\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [18:28, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.059157513082027435\n",
      "ppo/returns/mean: 0.5462327003479004\n",
      "ppo/policy/advantages_mean: 0.007264412939548492\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [18:48, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.019886992871761322\n",
      "ppo/returns/mean: 0.5484727621078491\n",
      "ppo/policy/advantages_mean: 0.005174936726689339\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [19:06, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.15200024843215942\n",
      "ppo/returns/mean: 0.548679530620575\n",
      "ppo/policy/advantages_mean: 0.0005625371704809368\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [19:22, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.07403381168842316\n",
      "ppo/returns/mean: 0.5488724112510681\n",
      "ppo/policy/advantages_mean: -0.003060923656448722\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [19:42, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.05367913842201233\n",
      "ppo/returns/mean: 0.5376991033554077\n",
      "ppo/policy/advantages_mean: 0.004013826604932547\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [19:59, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.03038051351904869\n",
      "ppo/returns/mean: 0.5573604106903076\n",
      "ppo/policy/advantages_mean: 0.0009522270411252975\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [20:14, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.0014824792742729187\n",
      "ppo/returns/mean: 0.5564751625061035\n",
      "ppo/policy/advantages_mean: -0.0011386226397007704\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [20:30, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.04659070819616318\n",
      "ppo/returns/mean: 0.5504528880119324\n",
      "ppo/policy/advantages_mean: -0.006692121736705303\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [20:48, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.09201271086931229\n",
      "ppo/returns/mean: 0.5490429401397705\n",
      "ppo/policy/advantages_mean: -0.006128481589257717\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [21:06, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.06070361286401749\n",
      "ppo/returns/mean: 0.5590073466300964\n",
      "ppo/policy/advantages_mean: -0.0016573065659031272\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [21:24, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.03430064767599106\n",
      "ppo/returns/mean: 0.5551648736000061\n",
      "ppo/policy/advantages_mean: 0.002447400940582156\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [21:42, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.06824635714292526\n",
      "ppo/returns/mean: 0.5516186952590942\n",
      "ppo/policy/advantages_mean: -0.004368680063635111\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [21:59, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.019973933696746826\n",
      "ppo/returns/mean: 0.5668906569480896\n",
      "ppo/policy/advantages_mean: 0.0025545943062752485\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [22:18, 18.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.015159569680690765\n",
      "ppo/returns/mean: 0.5665764212608337\n",
      "ppo/policy/advantages_mean: 0.00242402171716094\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [22:36, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.05545371025800705\n",
      "ppo/returns/mean: 0.5569909811019897\n",
      "ppo/policy/advantages_mean: 0.0012659328058362007\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [22:54, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.01855643279850483\n",
      "ppo/returns/mean: 0.5615234375\n",
      "ppo/policy/advantages_mean: -0.006460611708462238\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [23:12, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.11219315975904465\n",
      "ppo/returns/mean: 0.5545932650566101\n",
      "ppo/policy/advantages_mean: 0.00518676545470953\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [23:30, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.04506971687078476\n",
      "ppo/returns/mean: 0.562433660030365\n",
      "ppo/policy/advantages_mean: -0.0017066728323698044\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [23:45, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.09495706856250763\n",
      "ppo/returns/mean: 0.5601772665977478\n",
      "ppo/policy/advantages_mean: 0.0048878975212574005\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [24:02, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.027638882398605347\n",
      "ppo/returns/mean: 0.5567706823348999\n",
      "ppo/policy/advantages_mean: -0.005091940518468618\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [24:18, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.017184820026159286\n",
      "ppo/returns/mean: 0.5517389178276062\n",
      "ppo/policy/advantages_mean: 0.002571543212980032\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [24:34, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.08351362496614456\n",
      "ppo/returns/mean: 0.5540848970413208\n",
      "ppo/policy/advantages_mean: -0.00670247245579958\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [24:51, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.09037359058856964\n",
      "ppo/returns/mean: 0.5606344938278198\n",
      "ppo/policy/advantages_mean: -0.0025991201400756836\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [25:09, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.007849142886698246\n",
      "ppo/returns/mean: 0.5723594427108765\n",
      "ppo/policy/advantages_mean: -0.0017869044095277786\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [25:25, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.05883844196796417\n",
      "ppo/returns/mean: 0.5611841678619385\n",
      "ppo/policy/advantages_mean: -0.003919322043657303\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [25:43, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.1022462546825409\n",
      "ppo/returns/mean: 0.5655121803283691\n",
      "ppo/policy/advantages_mean: -0.007099844049662352\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [25:59, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.08246717602014542\n",
      "ppo/returns/mean: 0.5580060482025146\n",
      "ppo/policy/advantages_mean: -0.0024183127097785473\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [26:16, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.05359852313995361\n",
      "ppo/returns/mean: 0.5517401695251465\n",
      "ppo/policy/advantages_mean: -0.0005698163877241313\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [26:35, 17.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.041515789926052094\n",
      "ppo/returns/mean: 0.5618153810501099\n",
      "ppo/policy/advantages_mean: -0.010931842029094696\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [26:54, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.043928734958171844\n",
      "ppo/returns/mean: 0.5648149847984314\n",
      "ppo/policy/advantages_mean: -0.001079285517334938\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [27:10, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.04435547813773155\n",
      "ppo/returns/mean: 0.5669693350791931\n",
      "ppo/policy/advantages_mean: -0.0014983447035774589\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [27:26, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.01745159551501274\n",
      "ppo/returns/mean: 0.5599812865257263\n",
      "ppo/policy/advantages_mean: -0.001669015153311193\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [27:43, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.019577058032155037\n",
      "ppo/returns/mean: 0.5756101012229919\n",
      "ppo/policy/advantages_mean: -0.0012546818470582366\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [28:02, 17.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.004033096134662628\n",
      "ppo/returns/mean: 0.5709392428398132\n",
      "ppo/policy/advantages_mean: -0.00309437932446599\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [28:19, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.027891982346773148\n",
      "ppo/returns/mean: 0.5759447813034058\n",
      "ppo/policy/advantages_mean: 0.006105329841375351\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [28:37, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.030279502272605896\n",
      "ppo/returns/mean: 0.5550879836082458\n",
      "ppo/policy/advantages_mean: 0.000660043559037149\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [28:56, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.04733710736036301\n",
      "ppo/returns/mean: 0.5625548958778381\n",
      "ppo/policy/advantages_mean: -0.013566880486905575\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [29:13, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.12012763321399689\n",
      "ppo/returns/mean: 0.5627652406692505\n",
      "ppo/policy/advantages_mean: 0.008588030003011227\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop inspired in the training loop shown by the huggingface examples here:\n",
    "# https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "# https://github.com/huggingface/trl/blob/main/tests/test_ppo_trainer.py\n",
    "# https://huggingface.co/docs/trl/using_llama_models\n",
    "# https://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-ppo-deeplearning-ai\n",
    "\n",
    "# This is a HACK... lets see how this works out. May casue bias or may help. The good side is that this, being constant, can effect some type of regularization, preventing the model from gravitating too much towards any specific pattern in the training data.  Just a thought.\n",
    "DEFAULT_REJECTED_SUMMARY_TEXT = \"This is a bad summary\"\n",
    "\n",
    "# Some initial values\n",
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "# These hyperparams guide the generation of the completion in the policy model. We could add other params like temperature.\n",
    "generation_kwargs = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "max_ppo_steps = 100\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when we reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break   \n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # HACK!!!\n",
    "    # Check if original_prompt_tensors is a list of lists\n",
    "    if isinstance(prompt_tensors, list) and all(isinstance(item, list) for item in prompt_tensors):\n",
    "        \n",
    "        # Verify if sequences have fixed or variable length\n",
    "        lengths = [len(seq) for seq in prompt_tensors]\n",
    "        unique_lengths = set(lengths)\n",
    "        \n",
    "        # If sequences have variable lengths, pad them\n",
    "        if len(unique_lengths) > 1:\n",
    "            max_length = max(unique_lengths)\n",
    "            original_prompt_tensors = [seq + [0] * (max_length - len(seq)) for seq in prompt_tensors]  # padding with zeros\n",
    "            \n",
    "        # Convert original_prompt_tensors to individual tensors\n",
    "        prompt_tensors = [torch.tensor(seq).to(device) for seq in prompt_tensors]\n",
    "    \n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "\n",
    "        prompt_tensor = torch.tensor(prompt_tensor).to(device)\n",
    "\n",
    "        max_new_tokens = output_length_sampler()        \n",
    "            \n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "\n",
    "        \n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    #==========================================================================================\n",
    "    batch[\"response\"] = [policy_tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    \n",
    "    chosen_summaries = batch[\"response\"]\n",
    "    # Since there are no actual rejected summaries, we use this dummy text.\n",
    "    rejected_summaries = [DEFAULT_REJECTED_SUMMARY_TEXT] * len(batch[\"response\"]) \n",
    "\n",
    "    reward_tensors = []\n",
    "\n",
    "    for chosen_summary, rejected_summary in zip(chosen_summaries, rejected_summaries):\n",
    "        chosen_score, _, _, _ = score_summaries(rm_model, rm_tokenizer, chosen_summary, rejected_summary)\n",
    "        reward_tensors.append(torch.tensor(chosen_score))\n",
    "    \n",
    "    # ======================================================================================== \n",
    "\n",
    "    # https://huggingface.co/docs/trl/trainer#trl.PPOTrainer\n",
    "    # Run PPO step.\n",
    "    # Returns:\n",
    "    #  all_logprobs (torch.FloatTensor): Log probabilities of the responses, shape (batch_size, response_length)\n",
    "    #  all_ref_logprobs (torch.FloatTensor): Log probabilities of the responses, shape (batch_size, response_length)\n",
    "    #  all_values (torch.FloatTensor): Values of the responses, shape (batch_size, response_length)\n",
    "    \n",
    "    # From the source code of ppo_trainer.py:\n",
    "        # @PPODecorators.empty_cuda_cache()\n",
    "        # def step(\n",
    "        #     self,\n",
    "        #     queries: List[torch.LongTensor],\n",
    "        #     responses: List[torch.LongTensor],\n",
    "        #     scores: List[torch.FloatTensor],\n",
    "        #     response_masks: Optional[List[torch.LongTensor]] = None,\n",
    "        # ):\n",
    "        #     \"\"\"\n",
    "        #     Run a PPO optimisation step given a list of queries, model responses, and rewards.\n",
    "\n",
    "        #     Args:\n",
    "        #         queries (List[`torch.LongTensor`]):\n",
    "        #             List of tensors containing the encoded queries of shape (`query_length`)\n",
    "        #         responses (List[`torch.LongTensor`]):\n",
    "        #             List of tensors containing the encoded responses of shape (`response_length`)\n",
    "        #         scores (List[`torch.FloatTensor`]):\n",
    "        #             List of tensors containing the scores.\n",
    "        #         response_masks (List[`torch.FloatTensor`], *optional*)):\n",
    "        #             List of tensors containing masks of the response tokens.\n",
    "\n",
    "        #     Returns:\n",
    "        #         `dict[str, Any]`: A summary of the training statistics\n",
    "\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    \n",
    "    \n",
    "    # https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "    # See https://medium.com/@ben.burtenshaw/using-transformer-reinforcement-learning-to-detoxify-generative-language-models-5198446d6786\n",
    "    \n",
    "    # From source code of ppo_trainer.py:\n",
    "        # stats = self.record_step_stats(\n",
    "        #     scores=scores,\n",
    "        #     logprobs=all_logprobs,\n",
    "        #     ref_logprobs=ref_logprobs,\n",
    "        #     non_score_reward=non_score_reward,\n",
    "        #     train_stats=train_stats,\n",
    "        #     kl_coef=self.kl_ctl.value,\n",
    "        #     masks=masks,\n",
    "        #     queries=queries,\n",
    "        #     responses=responses,\n",
    "        # )\n",
    "        \n",
    "        # Gather/Reduce stats from all processes\n",
    "            # if self.is_distributed:\n",
    "            #     stats = self.gather_stats(stats)\n",
    "            # stats = stats_to_np(stats)\n",
    "            # timing[\"time/ppo/calc_stats\"] = time.time() - t\n",
    "            # stats[\"ppo/learning_rate\"] = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # # Update the KL control - multiply the batch_size by the number of processes\n",
    "            # self.kl_ctl.update(\n",
    "            #     stats[\"objective/kl\"],\n",
    "            #     self.config.batch_size * self.accelerator.num_processes,\n",
    "            # )\n",
    "\n",
    "            # # Log the total ppo time\n",
    "            # timing[\"time/ppo/total\"] = time.time() - t0\n",
    "            # stats.update(timing)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}') # Measures how different the policy's action distribution after the update is from the action distribution before the update. PPO tries to make these changes very small to avoid sudden changes.\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}') # This is the average return achieved by the agent. Higher is better.\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}') # Measures how much better an action is than the average action at a given state.\n",
    "    print('-'.join('' for x in range(100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model_path = \"./model_ppo_jco_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_ppo_jco_v1\\\\tokenizer_config.json',\n",
       " './model_ppo_jco_v1\\\\special_tokens_map.json',\n",
       " './model_ppo_jco_v1\\\\spiece.model',\n",
       " './model_ppo_jco_v1\\\\added_tokens.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "ppo_model.save_pretrained(ppo_model_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "policy_tokenizer.save_pretrained(ppo_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead # https://huggingface.co/docs/trl/quickstart\n",
    "\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(ppo_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(ppo_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(prompt: str, model, tokenizer, generation_kwargs, output_length_sampler) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary for a given prompt using a trained policy model.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): The input text for which a summary needs to be generated.\n",
    "    - model: The trained policy model.\n",
    "    - tokenizer: The tokenizer used for the policy model.\n",
    "    - generation_kwargs (dict): Arguments used for response generation.\n",
    "    - output_length_sampler (func): Function to sample the length of the output.\n",
    "\n",
    "    Returns:\n",
    "    - str: Generated summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    prompt_tensor = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Ensure it's only one tensor and check its shape\n",
    "    assert prompt_tensor.dim() == 2, f\"Unexpected tensor shape: {prompt_tensor.shape}\"\n",
    "    \n",
    "    # Set the generation arguments\n",
    "    max_new_tokens = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "    \n",
    "    # Generate a summary\n",
    "    summary_tensor = model.generate(input_ids=prompt_tensor, **generation_kwargs)\n",
    "    \n",
    "    # Decode and return the summary\n",
    "    summary = tokenizer.decode(summary_tensor[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"SUBREDDIT: r/relationships TITLE: How do I/do I at all [20 F] tell my boyfriend [23 M] that I'm bisexual? POST: I've had two serious relationships prior to this one, both with women. They had no problem with me being bisexual and it was something known before the relationship -- my first girlfriend was also bisexual. I am now in a relationship with a guy. We've been exclusive for about a month. Having never faced this issue, I come to you, Reddit. Is this something that he needs to know? Is it really relevant to a hetero relationship, regardless of if one of the participants in the relationship is bisexual? If you guys think it is necessary, when do you think is the right time? I think my biggest fear is losing him because of it. I know that I should be with someone who is fine with who I am, but I really like the guy and I'd hate for my sexual orientation to be the thing that kills this.\"\n",
    "text = \"SUBREDDIT: r/legaladvice TITLE: What can I do legally to restore water to my condominium!? POST: Hi, I live in SE Michigan in a condominium complex. Our water was shut off due to non-payment. (we recieved no notice) and we had to pay all that was due ($1500) We payed this yesterday at 2, they said the water would be turned on immediately. It wasn't. It's now the next day. The lady in our assosciation keeps insisting that the water meter is in another condo. Which we can't access because the person living there is never there (it's being rented) Now we're stuck with no water, no shower, no teeth brushing, no toilets, and no food for certain meals.... Please help us... What can we do? We called the police and they say that we can file a civil report for the lady not doing her job...\"\n",
    "text = \"SUBREDDIT: r/relationships TITLE: To go or not to go? Old friend (f, 23) getting married, I (f 23) don't want to because I have to go from here in the Netherlands to USA. POST: So, I have had this friend for a long time and we have always been there for each other. But about 6 months ago I moved here to the Netherlands to be with my partner (m23). This is our first place together here and we had to buy our own furniture. Needless to say we don't really have any money for trips. My friend is getting married in March in the USA and I feel really guilty out of obligation but I really don't want to go. I don't have the money for it and I don't want to leave here and miss my partner. Reasons for not wanting to go: 1. Money 2. Missing my partner. 3. Being incredibly bored once I'm there! I won't have a car or a way to get around, so I'll just be sitting in my parents house all day. I know it's bad that I don't want to go, but I am just really dreading it. Reddit, what do I do?\"\n",
    "text = \"SUBREDDIT: r/Advice TITLE: Bike tour around the world? POST: Hi there redditors! First of all I'd like to apologize for my English, but as you will see (I hope not), I'm not a native speaker. I'm 23-year-old who recently graduated from university and just stared my first job. Now, you see, my job is interesting and all, but it's an office job and I feel I'm not suited for this. I'm the adventures type, I want something happening around me and going to work from 9 to 6 is just killing me. The one thing that I thought of is a bike trip mostly in Europe, Asia and North Africa. The problem is that I'm from a country with an average salary around 350 euros or 450 USD. My salary is a bit higher - around 450 euros, but still not enough according to what I read is needed for such a trip, witch is about 30000 USD. My question is if somebody has done something like this without any money and if they have some tips for me. I'm thinking about sleeping outdoors or helping some locals for food and a place to crash. Is this something that could work out? I'm planning to go with my girlfriend and I think not too many people would take us in. Any help would be greatly appreciated!\"\n",
    "text = \"SUBREDDIT: r/Parenting TITLE: Question about saying 'no' to 18 month old POST: When I tell my son 'no' to something that is either dangerous (like sitting on the arm of the couch or trying to climb onto the television) or something that is an unwanted behavior (biting, hitting etc.) he looks at me and giggles before continuing to do whatever the hell he wants to do. When my husband tells him 'no' he stops what he's doing and sometimes gets upset to the point of crying (I think because his feelings are hurt). I guess the question is, how do I get him to listen to me and not just to his father? I have tried to make my voice sound louder and more masculine, but that just makes him laugh even harder.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL;DR: man says 'no' to dangerous behavior, stops biting his fathers, but sometimes gets upset. working on making my voice feel more masculine because of poor child behavior.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"{task_prefix}{text}\"\n",
    "generated_summary = generate_summary(prompt, ppo_model, policy_tokenizer, generation_kwargs, output_length_sampler)\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the ValueHead to make it a regular Seq2Seq model\n",
    "\n",
    "By pruning the ValueHead we turn the RL'ed model into a regular Seq2Seq model and we can deploy.  \n",
    "\n",
    "When further RL cycles are needed, the prune model becomes the base model, and we repeat the process, this time using this 'RL'ed' model as the base and improving it with the new RL cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98250220c904ee8b73d51a2898d398d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "new_model = T5ForConditionalGeneration.from_pretrained('t5-base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "old_state_dict = ppo_model.state_dict()\n",
    "new_state_dict = new_model.state_dict()\n",
    "\n",
    "for name, param in old_state_dict.items():\n",
    "    if \"v_head\" not in name:  # excluding the ValueHead parameters\n",
    "        new_state_dict[name].copy_(param)\n",
    "\n",
    "new_model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_model_path = './model_base_t5_finetuned_rl'\n",
    "new_model.save_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = T5ForConditionalGeneration.from_pretrained(new_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference method 1 in pruned model: As used with the T5 model previous to RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bisexual man is bisexual, but he's not a bisexual. he's bisexual and his first girlfriend was bisexual. he's bisexual and he's bisexual.\n"
     ]
    }
   ],
   "source": [
    "task_prefix = \"summarize: \" \n",
    "\n",
    "text = \"SUBREDDIT: r/relationships TITLE: How do I/do I at all [20 F] tell my boyfriend [23 M] that I'm bisexual? POST: I've had two serious relationships prior to this one, both with women. They had no problem with me being bisexual and it was something known before the relationship -- my first girlfriend was also bisexual. I am now in a relationship with a guy. We've been exclusive for about a month. Having never faced this issue, I come to you, Reddit. Is this something that he needs to know? Is it really relevant to a hetero relationship, regardless of if one of the participants in the relationship is bisexual? If you guys think it is necessary, when do you think is the right time? I think my biggest fear is losing him because of it. I know that I should be with someone who is fine with who I am, but I really like the guy and I'd hate for my sexual orientation to be the thing that kills this.\"\n",
    "#text = \"SUBREDDIT: r/legaladvice TITLE: What can I do legally to restore water to my condominium!? POST: Hi, I live in SE Michigan in a condominium complex. Our water was shut off due to non-payment. (we recieved no notice) and we had to pay all that was due ($1500) We payed this yesterday at 2, they said the water would be turned on immediately. It wasn't. It's now the next day. The lady in our assosciation keeps insisting that the water meter is in another condo. Which we can't access because the person living there is never there (it's being rented) Now we're stuck with no water, no shower, no teeth brushing, no toilets, and no food for certain meals.... Please help us... What can we do? We called the police and they say that we can file a civil report for the lady not doing her job...\"\n",
    "prompt = f\"{task_prefix}{text}\"\n",
    "input_ids = policy_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = loaded_model.generate(input_ids, max_length=100).to(device)\n",
    "\n",
    "strOutput = policy_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(strOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference method 2 in pruned model: As used with the PPO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"summarize: \"\n",
    "\n",
    "# This is a HACK... lets see how this works out. May casue bias or may help. The good side is that this, being constant, can effect some type of regularization, preventing the model from gravitating too much towards any specific pattern in the training data.  Just a thought.\n",
    "DEFAULT_REJECTED_SUMMARY_TEXT = \"This is a bad summary\"\n",
    "\n",
    "# Some initial values\n",
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "# These hyperparams guide the generation of the completion in the policy model. We could add other params like temperature.\n",
    "generation_kwargs = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dizzy kurtish: \"i've had 2 serious relationships prior to this one, both with women\" her biggest fear is losing him because of it, she says. she says she wants to ensure he's also honest with him about split.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"{task_prefix}{text}\"\n",
    "generated_summary = generate_summary(prompt, loaded_model, policy_tokenizer, generation_kwargs, output_length_sampler)\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME VALIDATIONS FOR SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(prompt_tensor))\n",
    "print(prompt_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first batch\n",
    "first_batch = next(iter(ppo_trainer.dataloader))\n",
    "print(first_batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(ppo_trainer.dataloader):\n",
    "    # Only run one step for inspection.\n",
    "    if step >= 1:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "    print(type(prompt_tensors[0]))\n",
    "\n",
    "    # Your loop continues...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
